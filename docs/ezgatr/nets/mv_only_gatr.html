<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 14.7.0"/>
    <title>ezgatr.nets.mv_only_gatr API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#49483e}.pdoc-code{background:#272822; color:#f8f8f2}.pdoc-code .c{color:#75715e}.pdoc-code .err{color:#960050; background-color:#1e0010}.pdoc-code .esc{color:#f8f8f2}.pdoc-code .g{color:#f8f8f2}.pdoc-code .k{color:#66d9ef}.pdoc-code .l{color:#ae81ff}.pdoc-code .n{color:#f8f8f2}.pdoc-code .o{color:#f92672}.pdoc-code .x{color:#f8f8f2}.pdoc-code .p{color:#f8f8f2}.pdoc-code .ch{color:#75715e}.pdoc-code .cm{color:#75715e}.pdoc-code .cp{color:#75715e}.pdoc-code .cpf{color:#75715e}.pdoc-code .c1{color:#75715e}.pdoc-code .cs{color:#75715e}.pdoc-code .gd{color:#f92672}.pdoc-code .ge{color:#f8f8f2; font-style:italic}.pdoc-code .gr{color:#f8f8f2}.pdoc-code .gh{color:#f8f8f2}.pdoc-code .gi{color:#a6e22e}.pdoc-code .go{color:#66d9ef}.pdoc-code .gp{color:#f92672; font-weight:bold}.pdoc-code .gs{color:#f8f8f2; font-weight:bold}.pdoc-code .gu{color:#75715e}.pdoc-code .gt{color:#f8f8f2}.pdoc-code .kc{color:#66d9ef}.pdoc-code .kd{color:#66d9ef}.pdoc-code .kn{color:#f92672}.pdoc-code .kp{color:#66d9ef}.pdoc-code .kr{color:#66d9ef}.pdoc-code .kt{color:#66d9ef}.pdoc-code .ld{color:#e6db74}.pdoc-code .m{color:#ae81ff}.pdoc-code .s{color:#e6db74}.pdoc-code .na{color:#a6e22e}.pdoc-code .nb{color:#f8f8f2}.pdoc-code .nc{color:#a6e22e}.pdoc-code .no{color:#66d9ef}.pdoc-code .nd{color:#a6e22e}.pdoc-code .ni{color:#f8f8f2}.pdoc-code .ne{color:#a6e22e}.pdoc-code .nf{color:#a6e22e}.pdoc-code .nl{color:#f8f8f2}.pdoc-code .nn{color:#f8f8f2}.pdoc-code .nx{color:#a6e22e}.pdoc-code .py{color:#f8f8f2}.pdoc-code .nt{color:#f92672}.pdoc-code .nv{color:#f8f8f2}.pdoc-code .ow{color:#f92672}.pdoc-code .w{color:#f8f8f2}.pdoc-code .mb{color:#ae81ff}.pdoc-code .mf{color:#ae81ff}.pdoc-code .mh{color:#ae81ff}.pdoc-code .mi{color:#ae81ff}.pdoc-code .mo{color:#ae81ff}.pdoc-code .sa{color:#e6db74}.pdoc-code .sb{color:#e6db74}.pdoc-code .sc{color:#e6db74}.pdoc-code .dl{color:#e6db74}.pdoc-code .sd{color:#e6db74}.pdoc-code .s2{color:#e6db74}.pdoc-code .se{color:#ae81ff}.pdoc-code .sh{color:#e6db74}.pdoc-code .si{color:#e6db74}.pdoc-code .sx{color:#e6db74}.pdoc-code .sr{color:#e6db74}.pdoc-code .s1{color:#e6db74}.pdoc-code .ss{color:#e6db74}.pdoc-code .bp{color:#f8f8f2}.pdoc-code .fm{color:#a6e22e}.pdoc-code .vc{color:#f8f8f2}.pdoc-code .vg{color:#f8f8f2}.pdoc-code .vi{color:#f8f8f2}.pdoc-code .vm{color:#f8f8f2}</style>
    <style>/*! theme.css */:root{--pdoc-background:#212529;}.pdoc{--text:#f7f7f7;--muted:#9d9d9d;--link:#58a6ff;--link-hover:#3989ff;--code:#333;--active:#555;--accent:#292929;--accent2:#555;--nav-hover:rgba(0, 0, 0, 0.1);--name:#77C1FF;--def:#0cdd0c;--annotation:#00c037;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../nets.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;ezgatr.nets</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MVOnlyGATrConfig.__init__">MVOnlyGATrConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.num_layers">num_layers</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.size_context">size_context</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.size_channels_in">size_channels_in</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.size_channels_out">size_channels_out</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.size_channels_hidden">size_channels_hidden</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.size_channels_intermediate">size_channels_intermediate</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.attn_num_heads">attn_num_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.attn_kinds">attn_kinds</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.attn_dropout_p">attn_dropout_p</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.attn_is_causal">attn_is_causal</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.attn_scale">attn_scale</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.norm_eps">norm_eps</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.norm_channelwise_rescale">norm_channelwise_rescale</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrConfig.gelu_approximate">gelu_approximate</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MVOnlyGATrEmbedding">MVOnlyGATrEmbedding</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MVOnlyGATrEmbedding.__init__">MVOnlyGATrEmbedding</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrEmbedding.config">config</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrEmbedding.embedding">embedding</a>
                        </li>
                        <li>
                                <a class="function" href="#MVOnlyGATrEmbedding.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MVOnlyGATrBilinear">MVOnlyGATrBilinear</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MVOnlyGATrBilinear.__init__">MVOnlyGATrBilinear</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrBilinear.config">config</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrBilinear.proj_bil">proj_bil</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrBilinear.proj_out">proj_out</a>
                        </li>
                        <li>
                                <a class="function" href="#MVOnlyGATrBilinear.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MVOnlyGATrMLP">MVOnlyGATrMLP</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MVOnlyGATrMLP.__init__">MVOnlyGATrMLP</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrMLP.config">config</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrMLP.layer_norm">layer_norm</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrMLP.equi_bil">equi_bil</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrMLP.proj_out">proj_out</a>
                        </li>
                        <li>
                                <a class="function" href="#MVOnlyGATrMLP.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MVOnlyGATrAttention">MVOnlyGATrAttention</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MVOnlyGATrAttention.__init__">MVOnlyGATrAttention</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrAttention.config">config</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrAttention.layer_norm">layer_norm</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrAttention.attn_mix">attn_mix</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrAttention.proj_qkv">proj_qkv</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrAttention.proj_out">proj_out</a>
                        </li>
                        <li>
                                <a class="function" href="#MVOnlyGATrAttention.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MVOnlyGATrBlock">MVOnlyGATrBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MVOnlyGATrBlock.__init__">MVOnlyGATrBlock</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrBlock.config">config</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrBlock.layer_id">layer_id</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrBlock.mlp">mlp</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrBlock.attn">attn</a>
                        </li>
                        <li>
                                <a class="function" href="#MVOnlyGATrBlock.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MVOnlyGATrModel">MVOnlyGATrModel</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MVOnlyGATrModel.__init__">MVOnlyGATrModel</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrModel.config">config</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrModel.embedding">embedding</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrModel.blocks">blocks</a>
                        </li>
                        <li>
                                <a class="variable" href="#MVOnlyGATrModel.head">head</a>
                        </li>
                        <li>
                                <a class="function" href="#MVOnlyGATrModel.forward">forward</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../ezgatr.html">ezgatr</a><wbr>.<a href="./../nets.html">nets</a><wbr>.mv_only_gatr    </h1>


                        <input id="mod-mv_only_gatr-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-mv_only_gatr-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="kn">import</span> <span class="nn">math</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">reduce</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Literal</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a><span class="kn">import</span> <span class="nn">torch</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="kn">from</span> <span class="nn">einops</span> <span class="kn">import</span> <span class="n">rearrange</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="kn">from</span> <span class="nn">ezgatr.nn</span> <span class="kn">import</span> <span class="n">EquiLinear</span><span class="p">,</span> <span class="n">EquiRMSNorm</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a><span class="kn">from</span> <span class="nn">ezgatr.nn.functional</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a>    <span class="n">equi_geometric_attention</span><span class="p">,</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a>    <span class="n">equi_join</span><span class="p">,</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a>    <span class="n">geometric_product</span><span class="p">,</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a>    <span class="n">scaler_gated_gelu</span><span class="p">,</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a><span class="p">)</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a><span class="nd">@dataclass</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrConfig</span><span class="p">:</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Configuration class for the ``MVOnlyGATr`` model.</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a><span class="sd">    Parameters</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a><span class="sd">    ----------</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a><span class="sd">    num_layers : int, default to 4</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a><span class="sd">        Number of GATr blocks in the network.</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="sd">    size_context : int, default to 2048</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a><span class="sd">        Number of elements, e.g., number of points in a point cloud,</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a><span class="sd">        in the input sequence. This argument is not actually used in</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a><span class="sd">        the model, but is kept for compatibility.</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a><span class="sd">    size_channels_in : int, default to 1</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a><span class="sd">        Number of input channels.</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a><span class="sd">    size_channels_out : int, default to 1</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a><span class="sd">        Number of output channels.</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a><span class="sd">    size_channels_hidden : int, default to 32</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a><span class="sd">        Number of hidden representation channels throughout the network, i.e.,</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a><span class="sd">        the input/output number of channels of the next layer, block, or module.</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a><span class="sd">    size_channels_intermediate : int, default to 32</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a><span class="sd">        Number of intermediate channels for the geometric bilinear operation.</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a><span class="sd">        Must be even. This intermediate size should not be confused with the size</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a><span class="sd">        of hidden representations throughout the network. It only refers to the</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a><span class="sd">        hidden sizes used for the equivariant join and geometric product operations.</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a><span class="sd">    norm_eps : float, optional</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a><span class="sd">        Small value to prevent division by zero in the normalization layer.</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="sd">    norm_channelwise_rescale : bool, default to True</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a><span class="sd">        Apply learnable channel-wise rescaling weights to the normalized multi-vector</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="sd">        inputs. Initialized to ones if set to ``True``.</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a><span class="sd">    gelu_approximate : str, default to &quot;tanh&quot;</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a><span class="sd">        Approximation method for the GeLU function. Default to &quot;tanh&quot;.</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a>    <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a>    <span class="n">size_context</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a>    <span class="n">size_channels_in</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a>    <span class="n">size_channels_out</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a>    <span class="n">size_channels_hidden</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a>    <span class="n">size_channels_intermediate</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a>    <span class="n">attn_num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a>    <span class="n">attn_kinds</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;ipa&quot;</span><span class="p">,</span> <span class="s2">&quot;daa&quot;</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a>        <span class="n">default_factory</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;ipa&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;daa&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a>    <span class="p">)</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a>    <span class="n">attn_dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a>    <span class="n">attn_is_causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a>    <span class="n">attn_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a>    <span class="n">norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a>    <span class="n">norm_channelwise_rescale</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a>    <span class="n">gelu_approximate</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;tanh&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;tanh&quot;</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Embedding layer to project input number of channels to hidden channels.</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a><span class="sd">    This layer corresponds to the very first equivariant linear layer of the</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a><span class="sd">    original design mentioned in the GATr paper.</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a><span class="sd">    Parameters</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a><span class="sd">    ----------</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a><span class="sd">    config : MVOnlyGATrConfig</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a><span class="sd">        Configuration object for the model. See ``MVOnlyGATrConfig`` for more details.</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a>    <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a>    <span class="n">embedding</span><span class="p">:</span> <span class="n">EquiLinear</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_in</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a>        <span class="p">)</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a>        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_in</span><span class="p">:</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a>                <span class="sa">f</span><span class="s2">&quot;Input tensor has </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> channels, &quot;</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a>                <span class="sa">f</span><span class="s2">&quot;expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_in</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>            <span class="p">)</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrBilinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements the geometric bilinear sub-layer of the geometric MLP.</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a><span class="sd">    Geometric bilinear operation consists of geometric product and equivariant</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a><span class="sd">    join operations. The results of two operations are concatenated along the</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a><span class="sd">    hidden channel axis and passed through a final equivariant linear projection</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a><span class="sd">    before being passed to the next layer, block, or module.</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a><span class="sd">    In both geometric product and equivariant join operations, the input</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a><span class="sd">    multi-vectors are first projected to a hidden space with the same number of</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a><span class="sd">    channels, i.e., left and right. Then, the results of each operation are</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a><span class="sd">    derived from the interaction of left and right hidden representations, each</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a><span class="sd">    with half number of ``size_channels_intermediate``.</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a><span class="sd">    Parameters</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a><span class="sd">    ----------</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a><span class="sd">    config : MVOnlyGATrConfig</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a><span class="sd">        Configuration object for the model. See ``MVOnlyGATrConfig`` for more details.</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a>    <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>    <span class="n">proj_bil</span><span class="p">:</span> <span class="n">EquiLinear</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a>    <span class="n">proj_out</span><span class="p">:</span> <span class="n">EquiLinear</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a>        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_intermediate</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of hidden channels must be even.&quot;</span><span class="p">)</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_bil</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_intermediate</span> <span class="o">*</span> <span class="mi">2</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>        <span class="p">)</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_intermediate</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>        <span class="p">)</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass of the geometric bilinear sub-layer.</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a><span class="sd">        Parameters</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a><span class="sd">        ----------</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a><span class="sd">        x : torch.Tensor</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a><span class="sd">            Batch of input hidden multi-vector representation tensor.</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a><span class="sd">        reference : torch.Tensor, optional</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a><span class="sd">            Reference tensor for the equivariant join operation.</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a><span class="sd">        Returns</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a><span class="sd">        -------</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a><span class="sd">        torch.Tensor</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a><span class="sd">            Batch of output hidden multi-vector representation tensor of the</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a><span class="sd">            same number of hidden channels.</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>        <span class="n">size_inter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_intermediate</span> <span class="o">//</span> <span class="mi">2</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>        <span class="n">lg</span><span class="p">,</span> <span class="n">rg</span><span class="p">,</span> <span class="n">lj</span><span class="p">,</span> <span class="n">rj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">proj_bil</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">size_inter</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">geometric_product</span><span class="p">(</span><span class="n">lg</span><span class="p">,</span> <span class="n">rg</span><span class="p">),</span> <span class="n">equi_join</span><span class="p">(</span><span class="n">lj</span><span class="p">,</span> <span class="n">rj</span><span class="p">,</span> <span class="n">reference</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Geometric MLP block without scaler channels.</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a><span class="sd">    Here we fix the structure of the MLP block to be a single equivariant linear</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a><span class="sd">    projection followed by a gated GELU activation function. In addition, the</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a><span class="sd">    equivariant normalization layer can be configured to be learnable, so the</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a><span class="sd">    normalization layer needs to be included in the block instead of being shared</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a><span class="sd">    across the network.</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a><span class="sd">    Parameters</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a><span class="sd">    ----------</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a><span class="sd">    config : MVOnlyGATrConfig</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a><span class="sd">        Configuration object for the model. See ``MVOnlyGATrConfig`` for more details.</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>    <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>    <span class="n">layer_norm</span><span class="p">:</span> <span class="n">EquiRMSNorm</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>    <span class="n">equi_bil</span><span class="p">:</span> <span class="n">MVOnlyGATrBilinear</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>    <span class="n">proj_out</span><span class="p">:</span> <span class="n">EquiLinear</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">EquiRMSNorm</span><span class="p">(</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a>            <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_eps</span><span class="p">,</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a>            <span class="n">channelwise_rescale</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_channelwise_rescale</span><span class="p">,</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>        <span class="p">)</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">equi_bil</span> <span class="o">=</span> <span class="n">MVOnlyGATrBilinear</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a>        <span class="p">)</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass of the geometric MLP block.</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a><span class="sd">        Parameters</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a><span class="sd">        ----------</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a><span class="sd">        x : torch.Tensor</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a><span class="sd">            Batch of input hidden multi-vector representation tensor.</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a><span class="sd">        reference : torch.Tensor, optional</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a><span class="sd">            Reference tensor for the equivariant join operation.</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a><span class="sd">        Returns</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a><span class="sd">        -------</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a><span class="sd">        torch.Tensor</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a><span class="sd">            Batch of output hidden multi-vector representation tensor of the</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a><span class="sd">            same number of hidden channels.</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">equi_bil</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reference</span><span class="p">)</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span><span class="p">(</span><span class="n">scaler_gated_gelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gelu_approximate</span><span class="p">))</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">residual</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Geometric attention block without scaler channels.</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a><span class="sd">    The GATr attention calculation is slightly different from the original</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a><span class="sd">    transformers implementation in that each head has the sample number of</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a><span class="sd">    channels as the input tensor, instead of dividing into smaller chunks.</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a><span class="sd">    In this case, the final output linear transformation maps from</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a><span class="sd">    ``size_channels_hidden * attn_num_heads`` to ``size_channels_hidden``.</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a><span class="sd">    One additional note here is that the ``attn_mix`` parameter is a dictionary</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a><span class="sd">    of learnable weighting parameter **LOGITS** for each attention kind.</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a><span class="sd">    They will be exponentiated before being used in the attention calculation.</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a><span class="sd">    Parameters</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a><span class="sd">    ----------</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a><span class="sd">    config : MVOnlyGATrConfig</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a><span class="sd">        Configuration object for the model. See ``MVOnlyGATrConfig`` for more details.</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a>    <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a>    <span class="n">layer_norm</span><span class="p">:</span> <span class="n">EquiRMSNorm</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a>    <span class="n">attn_mix</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a>    <span class="n">proj_qkv</span><span class="p">:</span> <span class="n">EquiLinear</span>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">EquiRMSNorm</span><span class="p">(</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a>            <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_eps</span><span class="p">,</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a>            <span class="n">channelwise_rescale</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_channelwise_rescale</span><span class="p">,</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a>        <span class="p">)</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a>        <span class="c1"># The two dummy dimensions are for the sequence length</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a>        <span class="c1"># and blade dimension, respectively.</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a>        <span class="n">attn_mix_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attn_mix</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a>        <span class="k">for</span> <span class="n">kind</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_kinds</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a>            <span class="n">param</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">attn_mix_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attn_mix</span><span class="p">[</span><span class="n">kind</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;attn_mix_</span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_qkv</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a>        <span class="p">)</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span><span class="p">,</span>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a>        <span class="p">)</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos">287</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos">288</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass of the geometric attention block.</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos">289</span></a>
</span><span id="L-290"><a href="#L-290"><span class="linenos">290</span></a><span class="sd">        Parameters</span>
</span><span id="L-291"><a href="#L-291"><span class="linenos">291</span></a><span class="sd">        ----------</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos">292</span></a><span class="sd">        x : torch.Tensor</span>
</span><span id="L-293"><a href="#L-293"><span class="linenos">293</span></a><span class="sd">            Batch of input hidden multi-vector representation tensor.</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos">294</span></a><span class="sd">        attn_mask : torch.Tensor, optional</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos">295</span></a><span class="sd">            Attention mask tensor for the attention operation. Usually</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos">296</span></a><span class="sd">            used if any specific attention constraints are needed within</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos">297</span></a><span class="sd">            a single sequence, such as padding mask or for discriminating</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos">298</span></a><span class="sd">            different subsequences.</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos">299</span></a>
</span><span id="L-300"><a href="#L-300"><span class="linenos">300</span></a><span class="sd">        Returns</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos">301</span></a><span class="sd">        -------</span>
</span><span id="L-302"><a href="#L-302"><span class="linenos">302</span></a><span class="sd">        torch.Tensor</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos">303</span></a><span class="sd">            Batch of output hidden multi-vector representation tensor of the</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos">304</span></a><span class="sd">            same number of hidden channels.</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos">305</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos">306</span></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-307"><a href="#L-307"><span class="linenos">307</span></a>
</span><span id="L-308"><a href="#L-308"><span class="linenos">308</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos">309</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
</span><span id="L-310"><a href="#L-310"><span class="linenos">310</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">proj_qkv</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
</span><span id="L-311"><a href="#L-311"><span class="linenos">311</span></a>            <span class="s2">&quot;b t (qkv h c) k -&gt; qkv b h t c k&quot;</span><span class="p">,</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos">312</span></a>            <span class="n">qkv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="L-313"><a href="#L-313"><span class="linenos">313</span></a>            <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span><span class="p">,</span>
</span><span id="L-314"><a href="#L-314"><span class="linenos">314</span></a>            <span class="n">c</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos">315</span></a>        <span class="p">)</span>
</span><span id="L-316"><a href="#L-316"><span class="linenos">316</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">equi_geometric_attention</span><span class="p">(</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos">317</span></a>            <span class="n">q</span><span class="p">,</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos">318</span></a>            <span class="n">k</span><span class="p">,</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos">319</span></a>            <span class="n">v</span><span class="p">,</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos">320</span></a>            <span class="n">kinds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_kinds</span><span class="p">,</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos">321</span></a>            <span class="n">weight</span><span class="o">=</span><span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_mix</span><span class="o">.</span><span class="n">values</span><span class="p">()],</span>
</span><span id="L-322"><a href="#L-322"><span class="linenos">322</span></a>            <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos">323</span></a>            <span class="n">is_causal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_is_causal</span><span class="p">,</span>
</span><span id="L-324"><a href="#L-324"><span class="linenos">324</span></a>            <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_dropout_p</span><span class="p">,</span>
</span><span id="L-325"><a href="#L-325"><span class="linenos">325</span></a>            <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_scale</span><span class="p">,</span>
</span><span id="L-326"><a href="#L-326"><span class="linenos">326</span></a>        <span class="p">)</span>
</span><span id="L-327"><a href="#L-327"><span class="linenos">327</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;b h t c k -&gt; b t (h c) k&quot;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span><span class="p">)</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos">328</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-329"><a href="#L-329"><span class="linenos">329</span></a>
</span><span id="L-330"><a href="#L-330"><span class="linenos">330</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">residual</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos">331</span></a>
</span><span id="L-332"><a href="#L-332"><span class="linenos">332</span></a>
</span><span id="L-333"><a href="#L-333"><span class="linenos">333</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-334"><a href="#L-334"><span class="linenos">334</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;GATr block without scaler channels.</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos">335</span></a>
</span><span id="L-336"><a href="#L-336"><span class="linenos">336</span></a><span class="sd">    Parameters</span>
</span><span id="L-337"><a href="#L-337"><span class="linenos">337</span></a><span class="sd">    ----------</span>
</span><span id="L-338"><a href="#L-338"><span class="linenos">338</span></a><span class="sd">    config : MVOnlyGATrConfig</span>
</span><span id="L-339"><a href="#L-339"><span class="linenos">339</span></a><span class="sd">        Configuration object for the model. See ``MVOnlyGATrConfig`` for more details.</span>
</span><span id="L-340"><a href="#L-340"><span class="linenos">340</span></a><span class="sd">    layer_id : int</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos">341</span></a><span class="sd">        Index of the current block in the network.</span>
</span><span id="L-342"><a href="#L-342"><span class="linenos">342</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos">343</span></a>
</span><span id="L-344"><a href="#L-344"><span class="linenos">344</span></a>    <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos">345</span></a>    <span class="n">layer_id</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="L-346"><a href="#L-346"><span class="linenos">346</span></a>    <span class="n">mlp</span><span class="p">:</span> <span class="n">MVOnlyGATrMLP</span>
</span><span id="L-347"><a href="#L-347"><span class="linenos">347</span></a>    <span class="n">attn</span><span class="p">:</span> <span class="n">MVOnlyGATrAttention</span>
</span><span id="L-348"><a href="#L-348"><span class="linenos">348</span></a>
</span><span id="L-349"><a href="#L-349"><span class="linenos">349</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos">350</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-351"><a href="#L-351"><span class="linenos">351</span></a>
</span><span id="L-352"><a href="#L-352"><span class="linenos">352</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="L-353"><a href="#L-353"><span class="linenos">353</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_id</span> <span class="o">=</span> <span class="n">layer_id</span>
</span><span id="L-354"><a href="#L-354"><span class="linenos">354</span></a>
</span><span id="L-355"><a href="#L-355"><span class="linenos">355</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MVOnlyGATrMLP</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span id="L-356"><a href="#L-356"><span class="linenos">356</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">MVOnlyGATrAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos">357</span></a>
</span><span id="L-358"><a href="#L-358"><span class="linenos">358</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos">359</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-360"><a href="#L-360"><span class="linenos">360</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos">361</span></a>        <span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-362"><a href="#L-362"><span class="linenos">362</span></a>        <span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos">363</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-364"><a href="#L-364"><span class="linenos">364</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reference</span><span class="p">),</span> <span class="n">attn_mask</span><span class="p">)</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos">365</span></a>
</span><span id="L-366"><a href="#L-366"><span class="linenos">366</span></a>
</span><span id="L-367"><a href="#L-367"><span class="linenos">367</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-368"><a href="#L-368"><span class="linenos">368</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Multi-Vector only GATr model.</span>
</span><span id="L-369"><a href="#L-369"><span class="linenos">369</span></a>
</span><span id="L-370"><a href="#L-370"><span class="linenos">370</span></a><span class="sd">    Parameters</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos">371</span></a><span class="sd">    ----------</span>
</span><span id="L-372"><a href="#L-372"><span class="linenos">372</span></a><span class="sd">    config : MVOnlyGATrConfig</span>
</span><span id="L-373"><a href="#L-373"><span class="linenos">373</span></a><span class="sd">        Configuration object for the model. See ``MVOnlyGATrConfig`` for more details.</span>
</span><span id="L-374"><a href="#L-374"><span class="linenos">374</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos">375</span></a>
</span><span id="L-376"><a href="#L-376"><span class="linenos">376</span></a>    <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos">377</span></a>    <span class="n">embedding</span><span class="p">:</span> <span class="n">MVOnlyGATrEmbedding</span>
</span><span id="L-378"><a href="#L-378"><span class="linenos">378</span></a>    <span class="n">blocks</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos">379</span></a>    <span class="n">head</span><span class="p">:</span> <span class="n">EquiLinear</span>
</span><span id="L-380"><a href="#L-380"><span class="linenos">380</span></a>
</span><span id="L-381"><a href="#L-381"><span class="linenos">381</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-382"><a href="#L-382"><span class="linenos">382</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos">383</span></a>
</span><span id="L-384"><a href="#L-384"><span class="linenos">384</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="L-385"><a href="#L-385"><span class="linenos">385</span></a>
</span><span id="L-386"><a href="#L-386"><span class="linenos">386</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">MVOnlyGATrEmbedding</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span id="L-387"><a href="#L-387"><span class="linenos">387</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos">388</span></a>            <span class="n">MVOnlyGATrBlock</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos">389</span></a>        <span class="p">)</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos">390</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_out</span><span class="p">)</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos">391</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">)</span>
</span><span id="L-392"><a href="#L-392"><span class="linenos">392</span></a>
</span><span id="L-393"><a href="#L-393"><span class="linenos">393</span></a>    <span class="k">def</span> <span class="nf">_init_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-394"><a href="#L-394"><span class="linenos">394</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Slight adjustment to Kaiming init by down-scaling the weights</span>
</span><span id="L-395"><a href="#L-395"><span class="linenos">395</span></a><span class="sd">        by the number of encoder layers, following the GPT-2 paper.</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos">396</span></a>
</span><span id="L-397"><a href="#L-397"><span class="linenos">397</span></a><span class="sd">        Parameters</span>
</span><span id="L-398"><a href="#L-398"><span class="linenos">398</span></a><span class="sd">        ----------</span>
</span><span id="L-399"><a href="#L-399"><span class="linenos">399</span></a><span class="sd">        module : nn.Module</span>
</span><span id="L-400"><a href="#L-400"><span class="linenos">400</span></a><span class="sd">            Module to initialize.</span>
</span><span id="L-401"><a href="#L-401"><span class="linenos">401</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-402"><a href="#L-402"><span class="linenos">402</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">EquiLinear</span><span class="p">):</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos">403</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
</span><span id="L-404"><a href="#L-404"><span class="linenos">404</span></a>            <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">/=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos">405</span></a>            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-406"><a href="#L-406"><span class="linenos">406</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</span><span id="L-407"><a href="#L-407"><span class="linenos">407</span></a>
</span><span id="L-408"><a href="#L-408"><span class="linenos">408</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos">409</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-410"><a href="#L-410"><span class="linenos">410</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-411"><a href="#L-411"><span class="linenos">411</span></a>        <span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-412"><a href="#L-412"><span class="linenos">412</span></a>        <span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-413"><a href="#L-413"><span class="linenos">413</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos">414</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span>
</span><span id="L-415"><a href="#L-415"><span class="linenos">415</span></a>            <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">block</span><span class="p">:</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reference</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">),</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos">416</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">,</span>
</span><span id="L-417"><a href="#L-417"><span class="linenos">417</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos">418</span></a>        <span class="p">)</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos">419</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></pre></div>


            </section>
                <section id="MVOnlyGATrConfig">
                            <input id="MVOnlyGATrConfig-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
                    <div class="decorator">@dataclass</div>

    <span class="def">class</span>
    <span class="name">MVOnlyGATrConfig</span>:

                <label class="view-source-button" for="MVOnlyGATrConfig-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrConfig-20"><a href="#MVOnlyGATrConfig-20"><span class="linenos">20</span></a><span class="nd">@dataclass</span>
</span><span id="MVOnlyGATrConfig-21"><a href="#MVOnlyGATrConfig-21"><span class="linenos">21</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrConfig</span><span class="p">:</span>
</span><span id="MVOnlyGATrConfig-22"><a href="#MVOnlyGATrConfig-22"><span class="linenos">22</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Configuration class for the ``MVOnlyGATr`` model.</span>
</span><span id="MVOnlyGATrConfig-23"><a href="#MVOnlyGATrConfig-23"><span class="linenos">23</span></a>
</span><span id="MVOnlyGATrConfig-24"><a href="#MVOnlyGATrConfig-24"><span class="linenos">24</span></a><span class="sd">    Parameters</span>
</span><span id="MVOnlyGATrConfig-25"><a href="#MVOnlyGATrConfig-25"><span class="linenos">25</span></a><span class="sd">    ----------</span>
</span><span id="MVOnlyGATrConfig-26"><a href="#MVOnlyGATrConfig-26"><span class="linenos">26</span></a><span class="sd">    num_layers : int, default to 4</span>
</span><span id="MVOnlyGATrConfig-27"><a href="#MVOnlyGATrConfig-27"><span class="linenos">27</span></a><span class="sd">        Number of GATr blocks in the network.</span>
</span><span id="MVOnlyGATrConfig-28"><a href="#MVOnlyGATrConfig-28"><span class="linenos">28</span></a><span class="sd">    size_context : int, default to 2048</span>
</span><span id="MVOnlyGATrConfig-29"><a href="#MVOnlyGATrConfig-29"><span class="linenos">29</span></a><span class="sd">        Number of elements, e.g., number of points in a point cloud,</span>
</span><span id="MVOnlyGATrConfig-30"><a href="#MVOnlyGATrConfig-30"><span class="linenos">30</span></a><span class="sd">        in the input sequence. This argument is not actually used in</span>
</span><span id="MVOnlyGATrConfig-31"><a href="#MVOnlyGATrConfig-31"><span class="linenos">31</span></a><span class="sd">        the model, but is kept for compatibility.</span>
</span><span id="MVOnlyGATrConfig-32"><a href="#MVOnlyGATrConfig-32"><span class="linenos">32</span></a><span class="sd">    size_channels_in : int, default to 1</span>
</span><span id="MVOnlyGATrConfig-33"><a href="#MVOnlyGATrConfig-33"><span class="linenos">33</span></a><span class="sd">        Number of input channels.</span>
</span><span id="MVOnlyGATrConfig-34"><a href="#MVOnlyGATrConfig-34"><span class="linenos">34</span></a><span class="sd">    size_channels_out : int, default to 1</span>
</span><span id="MVOnlyGATrConfig-35"><a href="#MVOnlyGATrConfig-35"><span class="linenos">35</span></a><span class="sd">        Number of output channels.</span>
</span><span id="MVOnlyGATrConfig-36"><a href="#MVOnlyGATrConfig-36"><span class="linenos">36</span></a><span class="sd">    size_channels_hidden : int, default to 32</span>
</span><span id="MVOnlyGATrConfig-37"><a href="#MVOnlyGATrConfig-37"><span class="linenos">37</span></a><span class="sd">        Number of hidden representation channels throughout the network, i.e.,</span>
</span><span id="MVOnlyGATrConfig-38"><a href="#MVOnlyGATrConfig-38"><span class="linenos">38</span></a><span class="sd">        the input/output number of channels of the next layer, block, or module.</span>
</span><span id="MVOnlyGATrConfig-39"><a href="#MVOnlyGATrConfig-39"><span class="linenos">39</span></a><span class="sd">    size_channels_intermediate : int, default to 32</span>
</span><span id="MVOnlyGATrConfig-40"><a href="#MVOnlyGATrConfig-40"><span class="linenos">40</span></a><span class="sd">        Number of intermediate channels for the geometric bilinear operation.</span>
</span><span id="MVOnlyGATrConfig-41"><a href="#MVOnlyGATrConfig-41"><span class="linenos">41</span></a><span class="sd">        Must be even. This intermediate size should not be confused with the size</span>
</span><span id="MVOnlyGATrConfig-42"><a href="#MVOnlyGATrConfig-42"><span class="linenos">42</span></a><span class="sd">        of hidden representations throughout the network. It only refers to the</span>
</span><span id="MVOnlyGATrConfig-43"><a href="#MVOnlyGATrConfig-43"><span class="linenos">43</span></a><span class="sd">        hidden sizes used for the equivariant join and geometric product operations.</span>
</span><span id="MVOnlyGATrConfig-44"><a href="#MVOnlyGATrConfig-44"><span class="linenos">44</span></a><span class="sd">    norm_eps : float, optional</span>
</span><span id="MVOnlyGATrConfig-45"><a href="#MVOnlyGATrConfig-45"><span class="linenos">45</span></a><span class="sd">        Small value to prevent division by zero in the normalization layer.</span>
</span><span id="MVOnlyGATrConfig-46"><a href="#MVOnlyGATrConfig-46"><span class="linenos">46</span></a><span class="sd">    norm_channelwise_rescale : bool, default to True</span>
</span><span id="MVOnlyGATrConfig-47"><a href="#MVOnlyGATrConfig-47"><span class="linenos">47</span></a><span class="sd">        Apply learnable channel-wise rescaling weights to the normalized multi-vector</span>
</span><span id="MVOnlyGATrConfig-48"><a href="#MVOnlyGATrConfig-48"><span class="linenos">48</span></a><span class="sd">        inputs. Initialized to ones if set to ``True``.</span>
</span><span id="MVOnlyGATrConfig-49"><a href="#MVOnlyGATrConfig-49"><span class="linenos">49</span></a><span class="sd">    gelu_approximate : str, default to &quot;tanh&quot;</span>
</span><span id="MVOnlyGATrConfig-50"><a href="#MVOnlyGATrConfig-50"><span class="linenos">50</span></a><span class="sd">        Approximation method for the GeLU function. Default to &quot;tanh&quot;.</span>
</span><span id="MVOnlyGATrConfig-51"><a href="#MVOnlyGATrConfig-51"><span class="linenos">51</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrConfig-52"><a href="#MVOnlyGATrConfig-52"><span class="linenos">52</span></a>
</span><span id="MVOnlyGATrConfig-53"><a href="#MVOnlyGATrConfig-53"><span class="linenos">53</span></a>    <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
</span><span id="MVOnlyGATrConfig-54"><a href="#MVOnlyGATrConfig-54"><span class="linenos">54</span></a>    <span class="n">size_context</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>
</span><span id="MVOnlyGATrConfig-55"><a href="#MVOnlyGATrConfig-55"><span class="linenos">55</span></a>
</span><span id="MVOnlyGATrConfig-56"><a href="#MVOnlyGATrConfig-56"><span class="linenos">56</span></a>    <span class="n">size_channels_in</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="MVOnlyGATrConfig-57"><a href="#MVOnlyGATrConfig-57"><span class="linenos">57</span></a>    <span class="n">size_channels_out</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="MVOnlyGATrConfig-58"><a href="#MVOnlyGATrConfig-58"><span class="linenos">58</span></a>    <span class="n">size_channels_hidden</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
</span><span id="MVOnlyGATrConfig-59"><a href="#MVOnlyGATrConfig-59"><span class="linenos">59</span></a>    <span class="n">size_channels_intermediate</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
</span><span id="MVOnlyGATrConfig-60"><a href="#MVOnlyGATrConfig-60"><span class="linenos">60</span></a>
</span><span id="MVOnlyGATrConfig-61"><a href="#MVOnlyGATrConfig-61"><span class="linenos">61</span></a>    <span class="n">attn_num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
</span><span id="MVOnlyGATrConfig-62"><a href="#MVOnlyGATrConfig-62"><span class="linenos">62</span></a>    <span class="n">attn_kinds</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;ipa&quot;</span><span class="p">,</span> <span class="s2">&quot;daa&quot;</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
</span><span id="MVOnlyGATrConfig-63"><a href="#MVOnlyGATrConfig-63"><span class="linenos">63</span></a>        <span class="n">default_factory</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;ipa&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;daa&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
</span><span id="MVOnlyGATrConfig-64"><a href="#MVOnlyGATrConfig-64"><span class="linenos">64</span></a>    <span class="p">)</span>
</span><span id="MVOnlyGATrConfig-65"><a href="#MVOnlyGATrConfig-65"><span class="linenos">65</span></a>    <span class="n">attn_dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>
</span><span id="MVOnlyGATrConfig-66"><a href="#MVOnlyGATrConfig-66"><span class="linenos">66</span></a>    <span class="n">attn_is_causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="MVOnlyGATrConfig-67"><a href="#MVOnlyGATrConfig-67"><span class="linenos">67</span></a>    <span class="n">attn_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MVOnlyGATrConfig-68"><a href="#MVOnlyGATrConfig-68"><span class="linenos">68</span></a>
</span><span id="MVOnlyGATrConfig-69"><a href="#MVOnlyGATrConfig-69"><span class="linenos">69</span></a>    <span class="n">norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MVOnlyGATrConfig-70"><a href="#MVOnlyGATrConfig-70"><span class="linenos">70</span></a>    <span class="n">norm_channelwise_rescale</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="MVOnlyGATrConfig-71"><a href="#MVOnlyGATrConfig-71"><span class="linenos">71</span></a>
</span><span id="MVOnlyGATrConfig-72"><a href="#MVOnlyGATrConfig-72"><span class="linenos">72</span></a>    <span class="n">gelu_approximate</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;tanh&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;tanh&quot;</span>
</span></pre></div>


            <div class="docstring"><p>Configuration class for the <code>MVOnlyGATr</code> model.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>num_layers</strong> (int, default to 4):
Number of GATr blocks in the network.</li>
<li><strong>size_context</strong> (int, default to 2048):
Number of elements, e.g., number of points in a point cloud,
in the input sequence. This argument is not actually used in
the model, but is kept for compatibility.</li>
<li><strong>size_channels_in</strong> (int, default to 1):
Number of input channels.</li>
<li><strong>size_channels_out</strong> (int, default to 1):
Number of output channels.</li>
<li><strong>size_channels_hidden</strong> (int, default to 32):
Number of hidden representation channels throughout the network, i.e.,
the input/output number of channels of the next layer, block, or module.</li>
<li><strong>size_channels_intermediate</strong> (int, default to 32):
Number of intermediate channels for the geometric bilinear operation.
Must be even. This intermediate size should not be confused with the size
of hidden representations throughout the network. It only refers to the
hidden sizes used for the equivariant join and geometric product operations.</li>
<li><strong>norm_eps</strong> (float, optional):
Small value to prevent division by zero in the normalization layer.</li>
<li><strong>norm_channelwise_rescale</strong> (bool, default to True):
Apply learnable channel-wise rescaling weights to the normalized multi-vector
inputs. Initialized to ones if set to <code>True</code>.</li>
<li><strong>gelu_approximate</strong> (str, default to "tanh"):
Approximation method for the GeLU function. Default to "tanh".</li>
</ul>
</div>


                            <div id="MVOnlyGATrConfig.__init__" class="classattr">
                                <div class="attr function">

        <span class="name">MVOnlyGATrConfig</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>,</span><span class="param">	<span class="n">size_context</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>,</span><span class="param">	<span class="n">size_channels_in</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>,</span><span class="param">	<span class="n">size_channels_out</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>,</span><span class="param">	<span class="n">size_channels_hidden</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>,</span><span class="param">	<span class="n">size_channels_intermediate</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>,</span><span class="param">	<span class="n">attn_num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>,</span><span class="param">	<span class="n">attn_kinds</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">typing</span><span class="o">.</span><span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;ipa&#39;</span><span class="p">,</span> <span class="s1">&#39;daa&#39;</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">typing</span><span class="o">.</span><span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">factory</span><span class="o">&gt;</span>,</span><span class="param">	<span class="n">attn_dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>,</span><span class="param">	<span class="n">attn_is_causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">attn_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">norm_channelwise_rescale</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">gelu_approximate</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;tanh&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;tanh&#39;</span></span>)</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.__init__"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.num_layers" class="classattr">
                                <div class="attr variable">
            <span class="name">num_layers</span><span class="annotation">: int</span>        =
<span class="default_value">4</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.num_layers"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.size_context" class="classattr">
                                <div class="attr variable">
            <span class="name">size_context</span><span class="annotation">: int</span>        =
<span class="default_value">2048</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.size_context"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.size_channels_in" class="classattr">
                                <div class="attr variable">
            <span class="name">size_channels_in</span><span class="annotation">: int</span>        =
<span class="default_value">1</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.size_channels_in"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.size_channels_out" class="classattr">
                                <div class="attr variable">
            <span class="name">size_channels_out</span><span class="annotation">: int</span>        =
<span class="default_value">1</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.size_channels_out"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.size_channels_hidden" class="classattr">
                                <div class="attr variable">
            <span class="name">size_channels_hidden</span><span class="annotation">: int</span>        =
<span class="default_value">32</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.size_channels_hidden"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.size_channels_intermediate" class="classattr">
                                <div class="attr variable">
            <span class="name">size_channels_intermediate</span><span class="annotation">: int</span>        =
<span class="default_value">32</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.size_channels_intermediate"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.attn_num_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">attn_num_heads</span><span class="annotation">: int</span>        =
<span class="default_value">4</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.attn_num_heads"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.attn_kinds" class="classattr">
                                <div class="attr variable">
            <span class="name">attn_kinds</span><span class="annotation">: dict[typing.Literal[&#39;ipa&#39;, &#39;daa&#39;], dict[str, typing.Any] | None]</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.attn_kinds"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.attn_dropout_p" class="classattr">
                                <div class="attr variable">
            <span class="name">attn_dropout_p</span><span class="annotation">: float</span>        =
<span class="default_value">0.0</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.attn_dropout_p"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.attn_is_causal" class="classattr">
                                <div class="attr variable">
            <span class="name">attn_is_causal</span><span class="annotation">: bool</span>        =
<span class="default_value">True</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.attn_is_causal"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.attn_scale" class="classattr">
                                <div class="attr variable">
            <span class="name">attn_scale</span><span class="annotation">: float | None</span>        =
<span class="default_value">None</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.attn_scale"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.norm_eps" class="classattr">
                                <div class="attr variable">
            <span class="name">norm_eps</span><span class="annotation">: float | None</span>        =
<span class="default_value">None</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.norm_eps"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.norm_channelwise_rescale" class="classattr">
                                <div class="attr variable">
            <span class="name">norm_channelwise_rescale</span><span class="annotation">: bool</span>        =
<span class="default_value">True</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.norm_channelwise_rescale"></a>



                            </div>
                            <div id="MVOnlyGATrConfig.gelu_approximate" class="classattr">
                                <div class="attr variable">
            <span class="name">gelu_approximate</span><span class="annotation">: Literal[&#39;none&#39;, &#39;tanh&#39;]</span>        =
<span class="default_value">&#39;tanh&#39;</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrConfig.gelu_approximate"></a>



                            </div>
                </section>
                <section id="MVOnlyGATrEmbedding">
                            <input id="MVOnlyGATrEmbedding-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">

    <span class="def">class</span>
    <span class="name">MVOnlyGATrEmbedding</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="MVOnlyGATrEmbedding-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrEmbedding"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrEmbedding-75"><a href="#MVOnlyGATrEmbedding-75"><span class="linenos"> 75</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="MVOnlyGATrEmbedding-76"><a href="#MVOnlyGATrEmbedding-76"><span class="linenos"> 76</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Embedding layer to project input number of channels to hidden channels.</span>
</span><span id="MVOnlyGATrEmbedding-77"><a href="#MVOnlyGATrEmbedding-77"><span class="linenos"> 77</span></a>
</span><span id="MVOnlyGATrEmbedding-78"><a href="#MVOnlyGATrEmbedding-78"><span class="linenos"> 78</span></a><span class="sd">    This layer corresponds to the very first equivariant linear layer of the</span>
</span><span id="MVOnlyGATrEmbedding-79"><a href="#MVOnlyGATrEmbedding-79"><span class="linenos"> 79</span></a><span class="sd">    original design mentioned in the GATr paper.</span>
</span><span id="MVOnlyGATrEmbedding-80"><a href="#MVOnlyGATrEmbedding-80"><span class="linenos"> 80</span></a>
</span><span id="MVOnlyGATrEmbedding-81"><a href="#MVOnlyGATrEmbedding-81"><span class="linenos"> 81</span></a><span class="sd">    Parameters</span>
</span><span id="MVOnlyGATrEmbedding-82"><a href="#MVOnlyGATrEmbedding-82"><span class="linenos"> 82</span></a><span class="sd">    ----------</span>
</span><span id="MVOnlyGATrEmbedding-83"><a href="#MVOnlyGATrEmbedding-83"><span class="linenos"> 83</span></a><span class="sd">    config : MVOnlyGATrConfig</span>
</span><span id="MVOnlyGATrEmbedding-84"><a href="#MVOnlyGATrEmbedding-84"><span class="linenos"> 84</span></a><span class="sd">        Configuration object for the model. See ``MVOnlyGATrConfig`` for more details.</span>
</span><span id="MVOnlyGATrEmbedding-85"><a href="#MVOnlyGATrEmbedding-85"><span class="linenos"> 85</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrEmbedding-86"><a href="#MVOnlyGATrEmbedding-86"><span class="linenos"> 86</span></a>
</span><span id="MVOnlyGATrEmbedding-87"><a href="#MVOnlyGATrEmbedding-87"><span class="linenos"> 87</span></a>    <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span>
</span><span id="MVOnlyGATrEmbedding-88"><a href="#MVOnlyGATrEmbedding-88"><span class="linenos"> 88</span></a>    <span class="n">embedding</span><span class="p">:</span> <span class="n">EquiLinear</span>
</span><span id="MVOnlyGATrEmbedding-89"><a href="#MVOnlyGATrEmbedding-89"><span class="linenos"> 89</span></a>
</span><span id="MVOnlyGATrEmbedding-90"><a href="#MVOnlyGATrEmbedding-90"><span class="linenos"> 90</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrEmbedding-91"><a href="#MVOnlyGATrEmbedding-91"><span class="linenos"> 91</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MVOnlyGATrEmbedding-92"><a href="#MVOnlyGATrEmbedding-92"><span class="linenos"> 92</span></a>
</span><span id="MVOnlyGATrEmbedding-93"><a href="#MVOnlyGATrEmbedding-93"><span class="linenos"> 93</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MVOnlyGATrEmbedding-94"><a href="#MVOnlyGATrEmbedding-94"><span class="linenos"> 94</span></a>
</span><span id="MVOnlyGATrEmbedding-95"><a href="#MVOnlyGATrEmbedding-95"><span class="linenos"> 95</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="MVOnlyGATrEmbedding-96"><a href="#MVOnlyGATrEmbedding-96"><span class="linenos"> 96</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_in</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span>
</span><span id="MVOnlyGATrEmbedding-97"><a href="#MVOnlyGATrEmbedding-97"><span class="linenos"> 97</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrEmbedding-98"><a href="#MVOnlyGATrEmbedding-98"><span class="linenos"> 98</span></a>
</span><span id="MVOnlyGATrEmbedding-99"><a href="#MVOnlyGATrEmbedding-99"><span class="linenos"> 99</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MVOnlyGATrEmbedding-100"><a href="#MVOnlyGATrEmbedding-100"><span class="linenos">100</span></a>        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_in</span><span class="p">:</span>
</span><span id="MVOnlyGATrEmbedding-101"><a href="#MVOnlyGATrEmbedding-101"><span class="linenos">101</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="MVOnlyGATrEmbedding-102"><a href="#MVOnlyGATrEmbedding-102"><span class="linenos">102</span></a>                <span class="sa">f</span><span class="s2">&quot;Input tensor has </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> channels, &quot;</span>
</span><span id="MVOnlyGATrEmbedding-103"><a href="#MVOnlyGATrEmbedding-103"><span class="linenos">103</span></a>                <span class="sa">f</span><span class="s2">&quot;expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_in</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="MVOnlyGATrEmbedding-104"><a href="#MVOnlyGATrEmbedding-104"><span class="linenos">104</span></a>            <span class="p">)</span>
</span><span id="MVOnlyGATrEmbedding-105"><a href="#MVOnlyGATrEmbedding-105"><span class="linenos">105</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Embedding layer to project input number of channels to hidden channels.</p>

<p>This layer corresponds to the very first equivariant linear layer of the
original design mentioned in the GATr paper.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>config</strong> (MVOnlyGATrConfig):
Configuration object for the model. See <code><a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></code> for more details.</li>
</ul>
</div>


                            <div id="MVOnlyGATrEmbedding.__init__" class="classattr">
                                        <input id="MVOnlyGATrEmbedding.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">

        <span class="name">MVOnlyGATrEmbedding</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">config</span><span class="p">:</span> <span class="n"><a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></span></span>)</span>

                <label class="view-source-button" for="MVOnlyGATrEmbedding.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrEmbedding.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrEmbedding.__init__-90"><a href="#MVOnlyGATrEmbedding.__init__-90"><span class="linenos">90</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrEmbedding.__init__-91"><a href="#MVOnlyGATrEmbedding.__init__-91"><span class="linenos">91</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MVOnlyGATrEmbedding.__init__-92"><a href="#MVOnlyGATrEmbedding.__init__-92"><span class="linenos">92</span></a>
</span><span id="MVOnlyGATrEmbedding.__init__-93"><a href="#MVOnlyGATrEmbedding.__init__-93"><span class="linenos">93</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MVOnlyGATrEmbedding.__init__-94"><a href="#MVOnlyGATrEmbedding.__init__-94"><span class="linenos">94</span></a>
</span><span id="MVOnlyGATrEmbedding.__init__-95"><a href="#MVOnlyGATrEmbedding.__init__-95"><span class="linenos">95</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="MVOnlyGATrEmbedding.__init__-96"><a href="#MVOnlyGATrEmbedding.__init__-96"><span class="linenos">96</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_in</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span>
</span><span id="MVOnlyGATrEmbedding.__init__-97"><a href="#MVOnlyGATrEmbedding.__init__-97"><span class="linenos">97</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="MVOnlyGATrEmbedding.config" class="classattr">
                                <div class="attr variable">
            <span class="name">config</span><span class="annotation">: <a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrEmbedding.config"></a>



                            </div>
                            <div id="MVOnlyGATrEmbedding.embedding" class="classattr">
                                <div class="attr variable">
            <span class="name">embedding</span><span class="annotation">: <a href="../nn/modules.html#EquiLinear">ezgatr.nn.modules.EquiLinear</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrEmbedding.embedding"></a>



                            </div>
                            <div id="MVOnlyGATrEmbedding.forward" class="classattr">
                                        <input id="MVOnlyGATrEmbedding.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">

        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="MVOnlyGATrEmbedding.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrEmbedding.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrEmbedding.forward-99"><a href="#MVOnlyGATrEmbedding.forward-99"><span class="linenos"> 99</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MVOnlyGATrEmbedding.forward-100"><a href="#MVOnlyGATrEmbedding.forward-100"><span class="linenos">100</span></a>        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_in</span><span class="p">:</span>
</span><span id="MVOnlyGATrEmbedding.forward-101"><a href="#MVOnlyGATrEmbedding.forward-101"><span class="linenos">101</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="MVOnlyGATrEmbedding.forward-102"><a href="#MVOnlyGATrEmbedding.forward-102"><span class="linenos">102</span></a>                <span class="sa">f</span><span class="s2">&quot;Input tensor has </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> channels, &quot;</span>
</span><span id="MVOnlyGATrEmbedding.forward-103"><a href="#MVOnlyGATrEmbedding.forward-103"><span class="linenos">103</span></a>                <span class="sa">f</span><span class="s2">&quot;expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_in</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="MVOnlyGATrEmbedding.forward-104"><a href="#MVOnlyGATrEmbedding.forward-104"><span class="linenos">104</span></a>            <span class="p">)</span>
</span><span id="MVOnlyGATrEmbedding.forward-105"><a href="#MVOnlyGATrEmbedding.forward-105"><span class="linenos">105</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="MVOnlyGATrEmbedding.dump_patches" class="variable">dump_patches</dd>
                <dd id="MVOnlyGATrEmbedding.training" class="variable">training</dd>
                <dd id="MVOnlyGATrEmbedding.call_super_init" class="variable">call_super_init</dd>
                <dd id="MVOnlyGATrEmbedding.register_buffer" class="function">register_buffer</dd>
                <dd id="MVOnlyGATrEmbedding.register_parameter" class="function">register_parameter</dd>
                <dd id="MVOnlyGATrEmbedding.add_module" class="function">add_module</dd>
                <dd id="MVOnlyGATrEmbedding.register_module" class="function">register_module</dd>
                <dd id="MVOnlyGATrEmbedding.get_submodule" class="function">get_submodule</dd>
                <dd id="MVOnlyGATrEmbedding.get_parameter" class="function">get_parameter</dd>
                <dd id="MVOnlyGATrEmbedding.get_buffer" class="function">get_buffer</dd>
                <dd id="MVOnlyGATrEmbedding.get_extra_state" class="function">get_extra_state</dd>
                <dd id="MVOnlyGATrEmbedding.set_extra_state" class="function">set_extra_state</dd>
                <dd id="MVOnlyGATrEmbedding.apply" class="function">apply</dd>
                <dd id="MVOnlyGATrEmbedding.cuda" class="function">cuda</dd>
                <dd id="MVOnlyGATrEmbedding.ipu" class="function">ipu</dd>
                <dd id="MVOnlyGATrEmbedding.xpu" class="function">xpu</dd>
                <dd id="MVOnlyGATrEmbedding.cpu" class="function">cpu</dd>
                <dd id="MVOnlyGATrEmbedding.type" class="function">type</dd>
                <dd id="MVOnlyGATrEmbedding.float" class="function">float</dd>
                <dd id="MVOnlyGATrEmbedding.double" class="function">double</dd>
                <dd id="MVOnlyGATrEmbedding.half" class="function">half</dd>
                <dd id="MVOnlyGATrEmbedding.bfloat16" class="function">bfloat16</dd>
                <dd id="MVOnlyGATrEmbedding.to_empty" class="function">to_empty</dd>
                <dd id="MVOnlyGATrEmbedding.to" class="function">to</dd>
                <dd id="MVOnlyGATrEmbedding.register_full_backward_pre_hook" class="function">register_full_backward_pre_hook</dd>
                <dd id="MVOnlyGATrEmbedding.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="MVOnlyGATrEmbedding.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="MVOnlyGATrEmbedding.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="MVOnlyGATrEmbedding.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="MVOnlyGATrEmbedding.register_state_dict_pre_hook" class="function">register_state_dict_pre_hook</dd>
                <dd id="MVOnlyGATrEmbedding.state_dict" class="function">state_dict</dd>
                <dd id="MVOnlyGATrEmbedding.register_load_state_dict_post_hook" class="function">register_load_state_dict_post_hook</dd>
                <dd id="MVOnlyGATrEmbedding.load_state_dict" class="function">load_state_dict</dd>
                <dd id="MVOnlyGATrEmbedding.parameters" class="function">parameters</dd>
                <dd id="MVOnlyGATrEmbedding.named_parameters" class="function">named_parameters</dd>
                <dd id="MVOnlyGATrEmbedding.buffers" class="function">buffers</dd>
                <dd id="MVOnlyGATrEmbedding.named_buffers" class="function">named_buffers</dd>
                <dd id="MVOnlyGATrEmbedding.children" class="function">children</dd>
                <dd id="MVOnlyGATrEmbedding.named_children" class="function">named_children</dd>
                <dd id="MVOnlyGATrEmbedding.modules" class="function">modules</dd>
                <dd id="MVOnlyGATrEmbedding.named_modules" class="function">named_modules</dd>
                <dd id="MVOnlyGATrEmbedding.train" class="function">train</dd>
                <dd id="MVOnlyGATrEmbedding.eval" class="function">eval</dd>
                <dd id="MVOnlyGATrEmbedding.requires_grad_" class="function">requires_grad_</dd>
                <dd id="MVOnlyGATrEmbedding.zero_grad" class="function">zero_grad</dd>
                <dd id="MVOnlyGATrEmbedding.share_memory" class="function">share_memory</dd>
                <dd id="MVOnlyGATrEmbedding.extra_repr" class="function">extra_repr</dd>
                <dd id="MVOnlyGATrEmbedding.compile" class="function">compile</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="MVOnlyGATrBilinear">
                            <input id="MVOnlyGATrBilinear-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">

    <span class="def">class</span>
    <span class="name">MVOnlyGATrBilinear</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="MVOnlyGATrBilinear-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrBilinear"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrBilinear-108"><a href="#MVOnlyGATrBilinear-108"><span class="linenos">108</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrBilinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="MVOnlyGATrBilinear-109"><a href="#MVOnlyGATrBilinear-109"><span class="linenos">109</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements the geometric bilinear sub-layer of the geometric MLP.</span>
</span><span id="MVOnlyGATrBilinear-110"><a href="#MVOnlyGATrBilinear-110"><span class="linenos">110</span></a>
</span><span id="MVOnlyGATrBilinear-111"><a href="#MVOnlyGATrBilinear-111"><span class="linenos">111</span></a><span class="sd">    Geometric bilinear operation consists of geometric product and equivariant</span>
</span><span id="MVOnlyGATrBilinear-112"><a href="#MVOnlyGATrBilinear-112"><span class="linenos">112</span></a><span class="sd">    join operations. The results of two operations are concatenated along the</span>
</span><span id="MVOnlyGATrBilinear-113"><a href="#MVOnlyGATrBilinear-113"><span class="linenos">113</span></a><span class="sd">    hidden channel axis and passed through a final equivariant linear projection</span>
</span><span id="MVOnlyGATrBilinear-114"><a href="#MVOnlyGATrBilinear-114"><span class="linenos">114</span></a><span class="sd">    before being passed to the next layer, block, or module.</span>
</span><span id="MVOnlyGATrBilinear-115"><a href="#MVOnlyGATrBilinear-115"><span class="linenos">115</span></a>
</span><span id="MVOnlyGATrBilinear-116"><a href="#MVOnlyGATrBilinear-116"><span class="linenos">116</span></a><span class="sd">    In both geometric product and equivariant join operations, the input</span>
</span><span id="MVOnlyGATrBilinear-117"><a href="#MVOnlyGATrBilinear-117"><span class="linenos">117</span></a><span class="sd">    multi-vectors are first projected to a hidden space with the same number of</span>
</span><span id="MVOnlyGATrBilinear-118"><a href="#MVOnlyGATrBilinear-118"><span class="linenos">118</span></a><span class="sd">    channels, i.e., left and right. Then, the results of each operation are</span>
</span><span id="MVOnlyGATrBilinear-119"><a href="#MVOnlyGATrBilinear-119"><span class="linenos">119</span></a><span class="sd">    derived from the interaction of left and right hidden representations, each</span>
</span><span id="MVOnlyGATrBilinear-120"><a href="#MVOnlyGATrBilinear-120"><span class="linenos">120</span></a><span class="sd">    with half number of ``size_channels_intermediate``.</span>
</span><span id="MVOnlyGATrBilinear-121"><a href="#MVOnlyGATrBilinear-121"><span class="linenos">121</span></a>
</span><span id="MVOnlyGATrBilinear-122"><a href="#MVOnlyGATrBilinear-122"><span class="linenos">122</span></a><span class="sd">    Parameters</span>
</span><span id="MVOnlyGATrBilinear-123"><a href="#MVOnlyGATrBilinear-123"><span class="linenos">123</span></a><span class="sd">    ----------</span>
</span><span id="MVOnlyGATrBilinear-124"><a href="#MVOnlyGATrBilinear-124"><span class="linenos">124</span></a><span class="sd">    config : MVOnlyGATrConfig</span>
</span><span id="MVOnlyGATrBilinear-125"><a href="#MVOnlyGATrBilinear-125"><span class="linenos">125</span></a><span class="sd">        Configuration object for the model. See ``MVOnlyGATrConfig`` for more details.</span>
</span><span id="MVOnlyGATrBilinear-126"><a href="#MVOnlyGATrBilinear-126"><span class="linenos">126</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrBilinear-127"><a href="#MVOnlyGATrBilinear-127"><span class="linenos">127</span></a>
</span><span id="MVOnlyGATrBilinear-128"><a href="#MVOnlyGATrBilinear-128"><span class="linenos">128</span></a>    <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span>
</span><span id="MVOnlyGATrBilinear-129"><a href="#MVOnlyGATrBilinear-129"><span class="linenos">129</span></a>    <span class="n">proj_bil</span><span class="p">:</span> <span class="n">EquiLinear</span>
</span><span id="MVOnlyGATrBilinear-130"><a href="#MVOnlyGATrBilinear-130"><span class="linenos">130</span></a>    <span class="n">proj_out</span><span class="p">:</span> <span class="n">EquiLinear</span>
</span><span id="MVOnlyGATrBilinear-131"><a href="#MVOnlyGATrBilinear-131"><span class="linenos">131</span></a>
</span><span id="MVOnlyGATrBilinear-132"><a href="#MVOnlyGATrBilinear-132"><span class="linenos">132</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrBilinear-133"><a href="#MVOnlyGATrBilinear-133"><span class="linenos">133</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MVOnlyGATrBilinear-134"><a href="#MVOnlyGATrBilinear-134"><span class="linenos">134</span></a>
</span><span id="MVOnlyGATrBilinear-135"><a href="#MVOnlyGATrBilinear-135"><span class="linenos">135</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MVOnlyGATrBilinear-136"><a href="#MVOnlyGATrBilinear-136"><span class="linenos">136</span></a>        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_intermediate</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="MVOnlyGATrBilinear-137"><a href="#MVOnlyGATrBilinear-137"><span class="linenos">137</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of hidden channels must be even.&quot;</span><span class="p">)</span>
</span><span id="MVOnlyGATrBilinear-138"><a href="#MVOnlyGATrBilinear-138"><span class="linenos">138</span></a>
</span><span id="MVOnlyGATrBilinear-139"><a href="#MVOnlyGATrBilinear-139"><span class="linenos">139</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_bil</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="MVOnlyGATrBilinear-140"><a href="#MVOnlyGATrBilinear-140"><span class="linenos">140</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_intermediate</span> <span class="o">*</span> <span class="mi">2</span>
</span><span id="MVOnlyGATrBilinear-141"><a href="#MVOnlyGATrBilinear-141"><span class="linenos">141</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrBilinear-142"><a href="#MVOnlyGATrBilinear-142"><span class="linenos">142</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="MVOnlyGATrBilinear-143"><a href="#MVOnlyGATrBilinear-143"><span class="linenos">143</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_intermediate</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span>
</span><span id="MVOnlyGATrBilinear-144"><a href="#MVOnlyGATrBilinear-144"><span class="linenos">144</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrBilinear-145"><a href="#MVOnlyGATrBilinear-145"><span class="linenos">145</span></a>
</span><span id="MVOnlyGATrBilinear-146"><a href="#MVOnlyGATrBilinear-146"><span class="linenos">146</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="MVOnlyGATrBilinear-147"><a href="#MVOnlyGATrBilinear-147"><span class="linenos">147</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MVOnlyGATrBilinear-148"><a href="#MVOnlyGATrBilinear-148"><span class="linenos">148</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MVOnlyGATrBilinear-149"><a href="#MVOnlyGATrBilinear-149"><span class="linenos">149</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass of the geometric bilinear sub-layer.</span>
</span><span id="MVOnlyGATrBilinear-150"><a href="#MVOnlyGATrBilinear-150"><span class="linenos">150</span></a>
</span><span id="MVOnlyGATrBilinear-151"><a href="#MVOnlyGATrBilinear-151"><span class="linenos">151</span></a><span class="sd">        Parameters</span>
</span><span id="MVOnlyGATrBilinear-152"><a href="#MVOnlyGATrBilinear-152"><span class="linenos">152</span></a><span class="sd">        ----------</span>
</span><span id="MVOnlyGATrBilinear-153"><a href="#MVOnlyGATrBilinear-153"><span class="linenos">153</span></a><span class="sd">        x : torch.Tensor</span>
</span><span id="MVOnlyGATrBilinear-154"><a href="#MVOnlyGATrBilinear-154"><span class="linenos">154</span></a><span class="sd">            Batch of input hidden multi-vector representation tensor.</span>
</span><span id="MVOnlyGATrBilinear-155"><a href="#MVOnlyGATrBilinear-155"><span class="linenos">155</span></a><span class="sd">        reference : torch.Tensor, optional</span>
</span><span id="MVOnlyGATrBilinear-156"><a href="#MVOnlyGATrBilinear-156"><span class="linenos">156</span></a><span class="sd">            Reference tensor for the equivariant join operation.</span>
</span><span id="MVOnlyGATrBilinear-157"><a href="#MVOnlyGATrBilinear-157"><span class="linenos">157</span></a>
</span><span id="MVOnlyGATrBilinear-158"><a href="#MVOnlyGATrBilinear-158"><span class="linenos">158</span></a><span class="sd">        Returns</span>
</span><span id="MVOnlyGATrBilinear-159"><a href="#MVOnlyGATrBilinear-159"><span class="linenos">159</span></a><span class="sd">        -------</span>
</span><span id="MVOnlyGATrBilinear-160"><a href="#MVOnlyGATrBilinear-160"><span class="linenos">160</span></a><span class="sd">        torch.Tensor</span>
</span><span id="MVOnlyGATrBilinear-161"><a href="#MVOnlyGATrBilinear-161"><span class="linenos">161</span></a><span class="sd">            Batch of output hidden multi-vector representation tensor of the</span>
</span><span id="MVOnlyGATrBilinear-162"><a href="#MVOnlyGATrBilinear-162"><span class="linenos">162</span></a><span class="sd">            same number of hidden channels.</span>
</span><span id="MVOnlyGATrBilinear-163"><a href="#MVOnlyGATrBilinear-163"><span class="linenos">163</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrBilinear-164"><a href="#MVOnlyGATrBilinear-164"><span class="linenos">164</span></a>        <span class="n">size_inter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_intermediate</span> <span class="o">//</span> <span class="mi">2</span>
</span><span id="MVOnlyGATrBilinear-165"><a href="#MVOnlyGATrBilinear-165"><span class="linenos">165</span></a>        <span class="n">lg</span><span class="p">,</span> <span class="n">rg</span><span class="p">,</span> <span class="n">lj</span><span class="p">,</span> <span class="n">rj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">proj_bil</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">size_inter</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span><span id="MVOnlyGATrBilinear-166"><a href="#MVOnlyGATrBilinear-166"><span class="linenos">166</span></a>
</span><span id="MVOnlyGATrBilinear-167"><a href="#MVOnlyGATrBilinear-167"><span class="linenos">167</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">geometric_product</span><span class="p">(</span><span class="n">lg</span><span class="p">,</span> <span class="n">rg</span><span class="p">),</span> <span class="n">equi_join</span><span class="p">(</span><span class="n">lj</span><span class="p">,</span> <span class="n">rj</span><span class="p">,</span> <span class="n">reference</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span><span id="MVOnlyGATrBilinear-168"><a href="#MVOnlyGATrBilinear-168"><span class="linenos">168</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Implements the geometric bilinear sub-layer of the geometric MLP.</p>

<p>Geometric bilinear operation consists of geometric product and equivariant
join operations. The results of two operations are concatenated along the
hidden channel axis and passed through a final equivariant linear projection
before being passed to the next layer, block, or module.</p>

<p>In both geometric product and equivariant join operations, the input
multi-vectors are first projected to a hidden space with the same number of
channels, i.e., left and right. Then, the results of each operation are
derived from the interaction of left and right hidden representations, each
with half number of <code>size_channels_intermediate</code>.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>config</strong> (MVOnlyGATrConfig):
Configuration object for the model. See <code><a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></code> for more details.</li>
</ul>
</div>


                            <div id="MVOnlyGATrBilinear.__init__" class="classattr">
                                        <input id="MVOnlyGATrBilinear.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">

        <span class="name">MVOnlyGATrBilinear</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">config</span><span class="p">:</span> <span class="n"><a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></span></span>)</span>

                <label class="view-source-button" for="MVOnlyGATrBilinear.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrBilinear.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrBilinear.__init__-132"><a href="#MVOnlyGATrBilinear.__init__-132"><span class="linenos">132</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrBilinear.__init__-133"><a href="#MVOnlyGATrBilinear.__init__-133"><span class="linenos">133</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MVOnlyGATrBilinear.__init__-134"><a href="#MVOnlyGATrBilinear.__init__-134"><span class="linenos">134</span></a>
</span><span id="MVOnlyGATrBilinear.__init__-135"><a href="#MVOnlyGATrBilinear.__init__-135"><span class="linenos">135</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MVOnlyGATrBilinear.__init__-136"><a href="#MVOnlyGATrBilinear.__init__-136"><span class="linenos">136</span></a>        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_intermediate</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="MVOnlyGATrBilinear.__init__-137"><a href="#MVOnlyGATrBilinear.__init__-137"><span class="linenos">137</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of hidden channels must be even.&quot;</span><span class="p">)</span>
</span><span id="MVOnlyGATrBilinear.__init__-138"><a href="#MVOnlyGATrBilinear.__init__-138"><span class="linenos">138</span></a>
</span><span id="MVOnlyGATrBilinear.__init__-139"><a href="#MVOnlyGATrBilinear.__init__-139"><span class="linenos">139</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_bil</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="MVOnlyGATrBilinear.__init__-140"><a href="#MVOnlyGATrBilinear.__init__-140"><span class="linenos">140</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_intermediate</span> <span class="o">*</span> <span class="mi">2</span>
</span><span id="MVOnlyGATrBilinear.__init__-141"><a href="#MVOnlyGATrBilinear.__init__-141"><span class="linenos">141</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrBilinear.__init__-142"><a href="#MVOnlyGATrBilinear.__init__-142"><span class="linenos">142</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="MVOnlyGATrBilinear.__init__-143"><a href="#MVOnlyGATrBilinear.__init__-143"><span class="linenos">143</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_intermediate</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span>
</span><span id="MVOnlyGATrBilinear.__init__-144"><a href="#MVOnlyGATrBilinear.__init__-144"><span class="linenos">144</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="MVOnlyGATrBilinear.config" class="classattr">
                                <div class="attr variable">
            <span class="name">config</span><span class="annotation">: <a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrBilinear.config"></a>



                            </div>
                            <div id="MVOnlyGATrBilinear.proj_bil" class="classattr">
                                <div class="attr variable">
            <span class="name">proj_bil</span><span class="annotation">: <a href="../nn/modules.html#EquiLinear">ezgatr.nn.modules.EquiLinear</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrBilinear.proj_bil"></a>



                            </div>
                            <div id="MVOnlyGATrBilinear.proj_out" class="classattr">
                                <div class="attr variable">
            <span class="name">proj_out</span><span class="annotation">: <a href="../nn/modules.html#EquiLinear">ezgatr.nn.modules.EquiLinear</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrBilinear.proj_out"></a>



                            </div>
                            <div id="MVOnlyGATrBilinear.forward" class="classattr">
                                        <input id="MVOnlyGATrBilinear.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">

        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="MVOnlyGATrBilinear.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrBilinear.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrBilinear.forward-146"><a href="#MVOnlyGATrBilinear.forward-146"><span class="linenos">146</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="MVOnlyGATrBilinear.forward-147"><a href="#MVOnlyGATrBilinear.forward-147"><span class="linenos">147</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MVOnlyGATrBilinear.forward-148"><a href="#MVOnlyGATrBilinear.forward-148"><span class="linenos">148</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MVOnlyGATrBilinear.forward-149"><a href="#MVOnlyGATrBilinear.forward-149"><span class="linenos">149</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass of the geometric bilinear sub-layer.</span>
</span><span id="MVOnlyGATrBilinear.forward-150"><a href="#MVOnlyGATrBilinear.forward-150"><span class="linenos">150</span></a>
</span><span id="MVOnlyGATrBilinear.forward-151"><a href="#MVOnlyGATrBilinear.forward-151"><span class="linenos">151</span></a><span class="sd">        Parameters</span>
</span><span id="MVOnlyGATrBilinear.forward-152"><a href="#MVOnlyGATrBilinear.forward-152"><span class="linenos">152</span></a><span class="sd">        ----------</span>
</span><span id="MVOnlyGATrBilinear.forward-153"><a href="#MVOnlyGATrBilinear.forward-153"><span class="linenos">153</span></a><span class="sd">        x : torch.Tensor</span>
</span><span id="MVOnlyGATrBilinear.forward-154"><a href="#MVOnlyGATrBilinear.forward-154"><span class="linenos">154</span></a><span class="sd">            Batch of input hidden multi-vector representation tensor.</span>
</span><span id="MVOnlyGATrBilinear.forward-155"><a href="#MVOnlyGATrBilinear.forward-155"><span class="linenos">155</span></a><span class="sd">        reference : torch.Tensor, optional</span>
</span><span id="MVOnlyGATrBilinear.forward-156"><a href="#MVOnlyGATrBilinear.forward-156"><span class="linenos">156</span></a><span class="sd">            Reference tensor for the equivariant join operation.</span>
</span><span id="MVOnlyGATrBilinear.forward-157"><a href="#MVOnlyGATrBilinear.forward-157"><span class="linenos">157</span></a>
</span><span id="MVOnlyGATrBilinear.forward-158"><a href="#MVOnlyGATrBilinear.forward-158"><span class="linenos">158</span></a><span class="sd">        Returns</span>
</span><span id="MVOnlyGATrBilinear.forward-159"><a href="#MVOnlyGATrBilinear.forward-159"><span class="linenos">159</span></a><span class="sd">        -------</span>
</span><span id="MVOnlyGATrBilinear.forward-160"><a href="#MVOnlyGATrBilinear.forward-160"><span class="linenos">160</span></a><span class="sd">        torch.Tensor</span>
</span><span id="MVOnlyGATrBilinear.forward-161"><a href="#MVOnlyGATrBilinear.forward-161"><span class="linenos">161</span></a><span class="sd">            Batch of output hidden multi-vector representation tensor of the</span>
</span><span id="MVOnlyGATrBilinear.forward-162"><a href="#MVOnlyGATrBilinear.forward-162"><span class="linenos">162</span></a><span class="sd">            same number of hidden channels.</span>
</span><span id="MVOnlyGATrBilinear.forward-163"><a href="#MVOnlyGATrBilinear.forward-163"><span class="linenos">163</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrBilinear.forward-164"><a href="#MVOnlyGATrBilinear.forward-164"><span class="linenos">164</span></a>        <span class="n">size_inter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_intermediate</span> <span class="o">//</span> <span class="mi">2</span>
</span><span id="MVOnlyGATrBilinear.forward-165"><a href="#MVOnlyGATrBilinear.forward-165"><span class="linenos">165</span></a>        <span class="n">lg</span><span class="p">,</span> <span class="n">rg</span><span class="p">,</span> <span class="n">lj</span><span class="p">,</span> <span class="n">rj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">proj_bil</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">size_inter</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span><span id="MVOnlyGATrBilinear.forward-166"><a href="#MVOnlyGATrBilinear.forward-166"><span class="linenos">166</span></a>
</span><span id="MVOnlyGATrBilinear.forward-167"><a href="#MVOnlyGATrBilinear.forward-167"><span class="linenos">167</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">geometric_product</span><span class="p">(</span><span class="n">lg</span><span class="p">,</span> <span class="n">rg</span><span class="p">),</span> <span class="n">equi_join</span><span class="p">(</span><span class="n">lj</span><span class="p">,</span> <span class="n">rj</span><span class="p">,</span> <span class="n">reference</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span><span id="MVOnlyGATrBilinear.forward-168"><a href="#MVOnlyGATrBilinear.forward-168"><span class="linenos">168</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Forward pass of the geometric bilinear sub-layer.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>x</strong> (torch.Tensor):
Batch of input hidden multi-vector representation tensor.</li>
<li><strong>reference</strong> (torch.Tensor, optional):
Reference tensor for the equivariant join operation.</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>torch.Tensor</strong>: Batch of output hidden multi-vector representation tensor of the
same number of hidden channels.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="MVOnlyGATrBilinear.dump_patches" class="variable">dump_patches</dd>
                <dd id="MVOnlyGATrBilinear.training" class="variable">training</dd>
                <dd id="MVOnlyGATrBilinear.call_super_init" class="variable">call_super_init</dd>
                <dd id="MVOnlyGATrBilinear.register_buffer" class="function">register_buffer</dd>
                <dd id="MVOnlyGATrBilinear.register_parameter" class="function">register_parameter</dd>
                <dd id="MVOnlyGATrBilinear.add_module" class="function">add_module</dd>
                <dd id="MVOnlyGATrBilinear.register_module" class="function">register_module</dd>
                <dd id="MVOnlyGATrBilinear.get_submodule" class="function">get_submodule</dd>
                <dd id="MVOnlyGATrBilinear.get_parameter" class="function">get_parameter</dd>
                <dd id="MVOnlyGATrBilinear.get_buffer" class="function">get_buffer</dd>
                <dd id="MVOnlyGATrBilinear.get_extra_state" class="function">get_extra_state</dd>
                <dd id="MVOnlyGATrBilinear.set_extra_state" class="function">set_extra_state</dd>
                <dd id="MVOnlyGATrBilinear.apply" class="function">apply</dd>
                <dd id="MVOnlyGATrBilinear.cuda" class="function">cuda</dd>
                <dd id="MVOnlyGATrBilinear.ipu" class="function">ipu</dd>
                <dd id="MVOnlyGATrBilinear.xpu" class="function">xpu</dd>
                <dd id="MVOnlyGATrBilinear.cpu" class="function">cpu</dd>
                <dd id="MVOnlyGATrBilinear.type" class="function">type</dd>
                <dd id="MVOnlyGATrBilinear.float" class="function">float</dd>
                <dd id="MVOnlyGATrBilinear.double" class="function">double</dd>
                <dd id="MVOnlyGATrBilinear.half" class="function">half</dd>
                <dd id="MVOnlyGATrBilinear.bfloat16" class="function">bfloat16</dd>
                <dd id="MVOnlyGATrBilinear.to_empty" class="function">to_empty</dd>
                <dd id="MVOnlyGATrBilinear.to" class="function">to</dd>
                <dd id="MVOnlyGATrBilinear.register_full_backward_pre_hook" class="function">register_full_backward_pre_hook</dd>
                <dd id="MVOnlyGATrBilinear.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="MVOnlyGATrBilinear.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="MVOnlyGATrBilinear.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="MVOnlyGATrBilinear.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="MVOnlyGATrBilinear.register_state_dict_pre_hook" class="function">register_state_dict_pre_hook</dd>
                <dd id="MVOnlyGATrBilinear.state_dict" class="function">state_dict</dd>
                <dd id="MVOnlyGATrBilinear.register_load_state_dict_post_hook" class="function">register_load_state_dict_post_hook</dd>
                <dd id="MVOnlyGATrBilinear.load_state_dict" class="function">load_state_dict</dd>
                <dd id="MVOnlyGATrBilinear.parameters" class="function">parameters</dd>
                <dd id="MVOnlyGATrBilinear.named_parameters" class="function">named_parameters</dd>
                <dd id="MVOnlyGATrBilinear.buffers" class="function">buffers</dd>
                <dd id="MVOnlyGATrBilinear.named_buffers" class="function">named_buffers</dd>
                <dd id="MVOnlyGATrBilinear.children" class="function">children</dd>
                <dd id="MVOnlyGATrBilinear.named_children" class="function">named_children</dd>
                <dd id="MVOnlyGATrBilinear.modules" class="function">modules</dd>
                <dd id="MVOnlyGATrBilinear.named_modules" class="function">named_modules</dd>
                <dd id="MVOnlyGATrBilinear.train" class="function">train</dd>
                <dd id="MVOnlyGATrBilinear.eval" class="function">eval</dd>
                <dd id="MVOnlyGATrBilinear.requires_grad_" class="function">requires_grad_</dd>
                <dd id="MVOnlyGATrBilinear.zero_grad" class="function">zero_grad</dd>
                <dd id="MVOnlyGATrBilinear.share_memory" class="function">share_memory</dd>
                <dd id="MVOnlyGATrBilinear.extra_repr" class="function">extra_repr</dd>
                <dd id="MVOnlyGATrBilinear.compile" class="function">compile</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="MVOnlyGATrMLP">
                            <input id="MVOnlyGATrMLP-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">

    <span class="def">class</span>
    <span class="name">MVOnlyGATrMLP</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="MVOnlyGATrMLP-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrMLP"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrMLP-171"><a href="#MVOnlyGATrMLP-171"><span class="linenos">171</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="MVOnlyGATrMLP-172"><a href="#MVOnlyGATrMLP-172"><span class="linenos">172</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Geometric MLP block without scaler channels.</span>
</span><span id="MVOnlyGATrMLP-173"><a href="#MVOnlyGATrMLP-173"><span class="linenos">173</span></a>
</span><span id="MVOnlyGATrMLP-174"><a href="#MVOnlyGATrMLP-174"><span class="linenos">174</span></a><span class="sd">    Here we fix the structure of the MLP block to be a single equivariant linear</span>
</span><span id="MVOnlyGATrMLP-175"><a href="#MVOnlyGATrMLP-175"><span class="linenos">175</span></a><span class="sd">    projection followed by a gated GELU activation function. In addition, the</span>
</span><span id="MVOnlyGATrMLP-176"><a href="#MVOnlyGATrMLP-176"><span class="linenos">176</span></a><span class="sd">    equivariant normalization layer can be configured to be learnable, so the</span>
</span><span id="MVOnlyGATrMLP-177"><a href="#MVOnlyGATrMLP-177"><span class="linenos">177</span></a><span class="sd">    normalization layer needs to be included in the block instead of being shared</span>
</span><span id="MVOnlyGATrMLP-178"><a href="#MVOnlyGATrMLP-178"><span class="linenos">178</span></a><span class="sd">    across the network.</span>
</span><span id="MVOnlyGATrMLP-179"><a href="#MVOnlyGATrMLP-179"><span class="linenos">179</span></a>
</span><span id="MVOnlyGATrMLP-180"><a href="#MVOnlyGATrMLP-180"><span class="linenos">180</span></a><span class="sd">    Parameters</span>
</span><span id="MVOnlyGATrMLP-181"><a href="#MVOnlyGATrMLP-181"><span class="linenos">181</span></a><span class="sd">    ----------</span>
</span><span id="MVOnlyGATrMLP-182"><a href="#MVOnlyGATrMLP-182"><span class="linenos">182</span></a><span class="sd">    config : MVOnlyGATrConfig</span>
</span><span id="MVOnlyGATrMLP-183"><a href="#MVOnlyGATrMLP-183"><span class="linenos">183</span></a><span class="sd">        Configuration object for the model. See ``MVOnlyGATrConfig`` for more details.</span>
</span><span id="MVOnlyGATrMLP-184"><a href="#MVOnlyGATrMLP-184"><span class="linenos">184</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrMLP-185"><a href="#MVOnlyGATrMLP-185"><span class="linenos">185</span></a>
</span><span id="MVOnlyGATrMLP-186"><a href="#MVOnlyGATrMLP-186"><span class="linenos">186</span></a>    <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span>
</span><span id="MVOnlyGATrMLP-187"><a href="#MVOnlyGATrMLP-187"><span class="linenos">187</span></a>    <span class="n">layer_norm</span><span class="p">:</span> <span class="n">EquiRMSNorm</span>
</span><span id="MVOnlyGATrMLP-188"><a href="#MVOnlyGATrMLP-188"><span class="linenos">188</span></a>    <span class="n">equi_bil</span><span class="p">:</span> <span class="n">MVOnlyGATrBilinear</span>
</span><span id="MVOnlyGATrMLP-189"><a href="#MVOnlyGATrMLP-189"><span class="linenos">189</span></a>    <span class="n">proj_out</span><span class="p">:</span> <span class="n">EquiLinear</span>
</span><span id="MVOnlyGATrMLP-190"><a href="#MVOnlyGATrMLP-190"><span class="linenos">190</span></a>
</span><span id="MVOnlyGATrMLP-191"><a href="#MVOnlyGATrMLP-191"><span class="linenos">191</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrMLP-192"><a href="#MVOnlyGATrMLP-192"><span class="linenos">192</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MVOnlyGATrMLP-193"><a href="#MVOnlyGATrMLP-193"><span class="linenos">193</span></a>
</span><span id="MVOnlyGATrMLP-194"><a href="#MVOnlyGATrMLP-194"><span class="linenos">194</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MVOnlyGATrMLP-195"><a href="#MVOnlyGATrMLP-195"><span class="linenos">195</span></a>
</span><span id="MVOnlyGATrMLP-196"><a href="#MVOnlyGATrMLP-196"><span class="linenos">196</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">EquiRMSNorm</span><span class="p">(</span>
</span><span id="MVOnlyGATrMLP-197"><a href="#MVOnlyGATrMLP-197"><span class="linenos">197</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="MVOnlyGATrMLP-198"><a href="#MVOnlyGATrMLP-198"><span class="linenos">198</span></a>            <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_eps</span><span class="p">,</span>
</span><span id="MVOnlyGATrMLP-199"><a href="#MVOnlyGATrMLP-199"><span class="linenos">199</span></a>            <span class="n">channelwise_rescale</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_channelwise_rescale</span><span class="p">,</span>
</span><span id="MVOnlyGATrMLP-200"><a href="#MVOnlyGATrMLP-200"><span class="linenos">200</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrMLP-201"><a href="#MVOnlyGATrMLP-201"><span class="linenos">201</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">equi_bil</span> <span class="o">=</span> <span class="n">MVOnlyGATrBilinear</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span id="MVOnlyGATrMLP-202"><a href="#MVOnlyGATrMLP-202"><span class="linenos">202</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="MVOnlyGATrMLP-203"><a href="#MVOnlyGATrMLP-203"><span class="linenos">203</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span>
</span><span id="MVOnlyGATrMLP-204"><a href="#MVOnlyGATrMLP-204"><span class="linenos">204</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrMLP-205"><a href="#MVOnlyGATrMLP-205"><span class="linenos">205</span></a>
</span><span id="MVOnlyGATrMLP-206"><a href="#MVOnlyGATrMLP-206"><span class="linenos">206</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="MVOnlyGATrMLP-207"><a href="#MVOnlyGATrMLP-207"><span class="linenos">207</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MVOnlyGATrMLP-208"><a href="#MVOnlyGATrMLP-208"><span class="linenos">208</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MVOnlyGATrMLP-209"><a href="#MVOnlyGATrMLP-209"><span class="linenos">209</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass of the geometric MLP block.</span>
</span><span id="MVOnlyGATrMLP-210"><a href="#MVOnlyGATrMLP-210"><span class="linenos">210</span></a>
</span><span id="MVOnlyGATrMLP-211"><a href="#MVOnlyGATrMLP-211"><span class="linenos">211</span></a><span class="sd">        Parameters</span>
</span><span id="MVOnlyGATrMLP-212"><a href="#MVOnlyGATrMLP-212"><span class="linenos">212</span></a><span class="sd">        ----------</span>
</span><span id="MVOnlyGATrMLP-213"><a href="#MVOnlyGATrMLP-213"><span class="linenos">213</span></a><span class="sd">        x : torch.Tensor</span>
</span><span id="MVOnlyGATrMLP-214"><a href="#MVOnlyGATrMLP-214"><span class="linenos">214</span></a><span class="sd">            Batch of input hidden multi-vector representation tensor.</span>
</span><span id="MVOnlyGATrMLP-215"><a href="#MVOnlyGATrMLP-215"><span class="linenos">215</span></a><span class="sd">        reference : torch.Tensor, optional</span>
</span><span id="MVOnlyGATrMLP-216"><a href="#MVOnlyGATrMLP-216"><span class="linenos">216</span></a><span class="sd">            Reference tensor for the equivariant join operation.</span>
</span><span id="MVOnlyGATrMLP-217"><a href="#MVOnlyGATrMLP-217"><span class="linenos">217</span></a>
</span><span id="MVOnlyGATrMLP-218"><a href="#MVOnlyGATrMLP-218"><span class="linenos">218</span></a><span class="sd">        Returns</span>
</span><span id="MVOnlyGATrMLP-219"><a href="#MVOnlyGATrMLP-219"><span class="linenos">219</span></a><span class="sd">        -------</span>
</span><span id="MVOnlyGATrMLP-220"><a href="#MVOnlyGATrMLP-220"><span class="linenos">220</span></a><span class="sd">        torch.Tensor</span>
</span><span id="MVOnlyGATrMLP-221"><a href="#MVOnlyGATrMLP-221"><span class="linenos">221</span></a><span class="sd">            Batch of output hidden multi-vector representation tensor of the</span>
</span><span id="MVOnlyGATrMLP-222"><a href="#MVOnlyGATrMLP-222"><span class="linenos">222</span></a><span class="sd">            same number of hidden channels.</span>
</span><span id="MVOnlyGATrMLP-223"><a href="#MVOnlyGATrMLP-223"><span class="linenos">223</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrMLP-224"><a href="#MVOnlyGATrMLP-224"><span class="linenos">224</span></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="MVOnlyGATrMLP-225"><a href="#MVOnlyGATrMLP-225"><span class="linenos">225</span></a>
</span><span id="MVOnlyGATrMLP-226"><a href="#MVOnlyGATrMLP-226"><span class="linenos">226</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MVOnlyGATrMLP-227"><a href="#MVOnlyGATrMLP-227"><span class="linenos">227</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">equi_bil</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reference</span><span class="p">)</span>
</span><span id="MVOnlyGATrMLP-228"><a href="#MVOnlyGATrMLP-228"><span class="linenos">228</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span><span class="p">(</span><span class="n">scaler_gated_gelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gelu_approximate</span><span class="p">))</span>
</span><span id="MVOnlyGATrMLP-229"><a href="#MVOnlyGATrMLP-229"><span class="linenos">229</span></a>
</span><span id="MVOnlyGATrMLP-230"><a href="#MVOnlyGATrMLP-230"><span class="linenos">230</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">residual</span>
</span></pre></div>


            <div class="docstring"><p>Geometric MLP block without scaler channels.</p>

<p>Here we fix the structure of the MLP block to be a single equivariant linear
projection followed by a gated GELU activation function. In addition, the
equivariant normalization layer can be configured to be learnable, so the
normalization layer needs to be included in the block instead of being shared
across the network.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>config</strong> (MVOnlyGATrConfig):
Configuration object for the model. See <code><a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></code> for more details.</li>
</ul>
</div>


                            <div id="MVOnlyGATrMLP.__init__" class="classattr">
                                        <input id="MVOnlyGATrMLP.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">

        <span class="name">MVOnlyGATrMLP</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">config</span><span class="p">:</span> <span class="n"><a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></span></span>)</span>

                <label class="view-source-button" for="MVOnlyGATrMLP.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrMLP.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrMLP.__init__-191"><a href="#MVOnlyGATrMLP.__init__-191"><span class="linenos">191</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrMLP.__init__-192"><a href="#MVOnlyGATrMLP.__init__-192"><span class="linenos">192</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MVOnlyGATrMLP.__init__-193"><a href="#MVOnlyGATrMLP.__init__-193"><span class="linenos">193</span></a>
</span><span id="MVOnlyGATrMLP.__init__-194"><a href="#MVOnlyGATrMLP.__init__-194"><span class="linenos">194</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MVOnlyGATrMLP.__init__-195"><a href="#MVOnlyGATrMLP.__init__-195"><span class="linenos">195</span></a>
</span><span id="MVOnlyGATrMLP.__init__-196"><a href="#MVOnlyGATrMLP.__init__-196"><span class="linenos">196</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">EquiRMSNorm</span><span class="p">(</span>
</span><span id="MVOnlyGATrMLP.__init__-197"><a href="#MVOnlyGATrMLP.__init__-197"><span class="linenos">197</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="MVOnlyGATrMLP.__init__-198"><a href="#MVOnlyGATrMLP.__init__-198"><span class="linenos">198</span></a>            <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_eps</span><span class="p">,</span>
</span><span id="MVOnlyGATrMLP.__init__-199"><a href="#MVOnlyGATrMLP.__init__-199"><span class="linenos">199</span></a>            <span class="n">channelwise_rescale</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_channelwise_rescale</span><span class="p">,</span>
</span><span id="MVOnlyGATrMLP.__init__-200"><a href="#MVOnlyGATrMLP.__init__-200"><span class="linenos">200</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrMLP.__init__-201"><a href="#MVOnlyGATrMLP.__init__-201"><span class="linenos">201</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">equi_bil</span> <span class="o">=</span> <span class="n">MVOnlyGATrBilinear</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span id="MVOnlyGATrMLP.__init__-202"><a href="#MVOnlyGATrMLP.__init__-202"><span class="linenos">202</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="MVOnlyGATrMLP.__init__-203"><a href="#MVOnlyGATrMLP.__init__-203"><span class="linenos">203</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span>
</span><span id="MVOnlyGATrMLP.__init__-204"><a href="#MVOnlyGATrMLP.__init__-204"><span class="linenos">204</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="MVOnlyGATrMLP.config" class="classattr">
                                <div class="attr variable">
            <span class="name">config</span><span class="annotation">: <a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrMLP.config"></a>



                            </div>
                            <div id="MVOnlyGATrMLP.layer_norm" class="classattr">
                                <div class="attr variable">
            <span class="name">layer_norm</span><span class="annotation">: <a href="../nn/modules.html#EquiRMSNorm">ezgatr.nn.modules.EquiRMSNorm</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrMLP.layer_norm"></a>



                            </div>
                            <div id="MVOnlyGATrMLP.equi_bil" class="classattr">
                                <div class="attr variable">
            <span class="name">equi_bil</span><span class="annotation">: <a href="#MVOnlyGATrBilinear">MVOnlyGATrBilinear</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrMLP.equi_bil"></a>



                            </div>
                            <div id="MVOnlyGATrMLP.proj_out" class="classattr">
                                <div class="attr variable">
            <span class="name">proj_out</span><span class="annotation">: <a href="../nn/modules.html#EquiLinear">ezgatr.nn.modules.EquiLinear</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrMLP.proj_out"></a>



                            </div>
                            <div id="MVOnlyGATrMLP.forward" class="classattr">
                                        <input id="MVOnlyGATrMLP.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">

        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="MVOnlyGATrMLP.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrMLP.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrMLP.forward-206"><a href="#MVOnlyGATrMLP.forward-206"><span class="linenos">206</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="MVOnlyGATrMLP.forward-207"><a href="#MVOnlyGATrMLP.forward-207"><span class="linenos">207</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MVOnlyGATrMLP.forward-208"><a href="#MVOnlyGATrMLP.forward-208"><span class="linenos">208</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MVOnlyGATrMLP.forward-209"><a href="#MVOnlyGATrMLP.forward-209"><span class="linenos">209</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass of the geometric MLP block.</span>
</span><span id="MVOnlyGATrMLP.forward-210"><a href="#MVOnlyGATrMLP.forward-210"><span class="linenos">210</span></a>
</span><span id="MVOnlyGATrMLP.forward-211"><a href="#MVOnlyGATrMLP.forward-211"><span class="linenos">211</span></a><span class="sd">        Parameters</span>
</span><span id="MVOnlyGATrMLP.forward-212"><a href="#MVOnlyGATrMLP.forward-212"><span class="linenos">212</span></a><span class="sd">        ----------</span>
</span><span id="MVOnlyGATrMLP.forward-213"><a href="#MVOnlyGATrMLP.forward-213"><span class="linenos">213</span></a><span class="sd">        x : torch.Tensor</span>
</span><span id="MVOnlyGATrMLP.forward-214"><a href="#MVOnlyGATrMLP.forward-214"><span class="linenos">214</span></a><span class="sd">            Batch of input hidden multi-vector representation tensor.</span>
</span><span id="MVOnlyGATrMLP.forward-215"><a href="#MVOnlyGATrMLP.forward-215"><span class="linenos">215</span></a><span class="sd">        reference : torch.Tensor, optional</span>
</span><span id="MVOnlyGATrMLP.forward-216"><a href="#MVOnlyGATrMLP.forward-216"><span class="linenos">216</span></a><span class="sd">            Reference tensor for the equivariant join operation.</span>
</span><span id="MVOnlyGATrMLP.forward-217"><a href="#MVOnlyGATrMLP.forward-217"><span class="linenos">217</span></a>
</span><span id="MVOnlyGATrMLP.forward-218"><a href="#MVOnlyGATrMLP.forward-218"><span class="linenos">218</span></a><span class="sd">        Returns</span>
</span><span id="MVOnlyGATrMLP.forward-219"><a href="#MVOnlyGATrMLP.forward-219"><span class="linenos">219</span></a><span class="sd">        -------</span>
</span><span id="MVOnlyGATrMLP.forward-220"><a href="#MVOnlyGATrMLP.forward-220"><span class="linenos">220</span></a><span class="sd">        torch.Tensor</span>
</span><span id="MVOnlyGATrMLP.forward-221"><a href="#MVOnlyGATrMLP.forward-221"><span class="linenos">221</span></a><span class="sd">            Batch of output hidden multi-vector representation tensor of the</span>
</span><span id="MVOnlyGATrMLP.forward-222"><a href="#MVOnlyGATrMLP.forward-222"><span class="linenos">222</span></a><span class="sd">            same number of hidden channels.</span>
</span><span id="MVOnlyGATrMLP.forward-223"><a href="#MVOnlyGATrMLP.forward-223"><span class="linenos">223</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrMLP.forward-224"><a href="#MVOnlyGATrMLP.forward-224"><span class="linenos">224</span></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="MVOnlyGATrMLP.forward-225"><a href="#MVOnlyGATrMLP.forward-225"><span class="linenos">225</span></a>
</span><span id="MVOnlyGATrMLP.forward-226"><a href="#MVOnlyGATrMLP.forward-226"><span class="linenos">226</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MVOnlyGATrMLP.forward-227"><a href="#MVOnlyGATrMLP.forward-227"><span class="linenos">227</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">equi_bil</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reference</span><span class="p">)</span>
</span><span id="MVOnlyGATrMLP.forward-228"><a href="#MVOnlyGATrMLP.forward-228"><span class="linenos">228</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span><span class="p">(</span><span class="n">scaler_gated_gelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gelu_approximate</span><span class="p">))</span>
</span><span id="MVOnlyGATrMLP.forward-229"><a href="#MVOnlyGATrMLP.forward-229"><span class="linenos">229</span></a>
</span><span id="MVOnlyGATrMLP.forward-230"><a href="#MVOnlyGATrMLP.forward-230"><span class="linenos">230</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">residual</span>
</span></pre></div>


            <div class="docstring"><p>Forward pass of the geometric MLP block.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>x</strong> (torch.Tensor):
Batch of input hidden multi-vector representation tensor.</li>
<li><strong>reference</strong> (torch.Tensor, optional):
Reference tensor for the equivariant join operation.</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>torch.Tensor</strong>: Batch of output hidden multi-vector representation tensor of the
same number of hidden channels.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="MVOnlyGATrMLP.dump_patches" class="variable">dump_patches</dd>
                <dd id="MVOnlyGATrMLP.training" class="variable">training</dd>
                <dd id="MVOnlyGATrMLP.call_super_init" class="variable">call_super_init</dd>
                <dd id="MVOnlyGATrMLP.register_buffer" class="function">register_buffer</dd>
                <dd id="MVOnlyGATrMLP.register_parameter" class="function">register_parameter</dd>
                <dd id="MVOnlyGATrMLP.add_module" class="function">add_module</dd>
                <dd id="MVOnlyGATrMLP.register_module" class="function">register_module</dd>
                <dd id="MVOnlyGATrMLP.get_submodule" class="function">get_submodule</dd>
                <dd id="MVOnlyGATrMLP.get_parameter" class="function">get_parameter</dd>
                <dd id="MVOnlyGATrMLP.get_buffer" class="function">get_buffer</dd>
                <dd id="MVOnlyGATrMLP.get_extra_state" class="function">get_extra_state</dd>
                <dd id="MVOnlyGATrMLP.set_extra_state" class="function">set_extra_state</dd>
                <dd id="MVOnlyGATrMLP.apply" class="function">apply</dd>
                <dd id="MVOnlyGATrMLP.cuda" class="function">cuda</dd>
                <dd id="MVOnlyGATrMLP.ipu" class="function">ipu</dd>
                <dd id="MVOnlyGATrMLP.xpu" class="function">xpu</dd>
                <dd id="MVOnlyGATrMLP.cpu" class="function">cpu</dd>
                <dd id="MVOnlyGATrMLP.type" class="function">type</dd>
                <dd id="MVOnlyGATrMLP.float" class="function">float</dd>
                <dd id="MVOnlyGATrMLP.double" class="function">double</dd>
                <dd id="MVOnlyGATrMLP.half" class="function">half</dd>
                <dd id="MVOnlyGATrMLP.bfloat16" class="function">bfloat16</dd>
                <dd id="MVOnlyGATrMLP.to_empty" class="function">to_empty</dd>
                <dd id="MVOnlyGATrMLP.to" class="function">to</dd>
                <dd id="MVOnlyGATrMLP.register_full_backward_pre_hook" class="function">register_full_backward_pre_hook</dd>
                <dd id="MVOnlyGATrMLP.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="MVOnlyGATrMLP.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="MVOnlyGATrMLP.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="MVOnlyGATrMLP.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="MVOnlyGATrMLP.register_state_dict_pre_hook" class="function">register_state_dict_pre_hook</dd>
                <dd id="MVOnlyGATrMLP.state_dict" class="function">state_dict</dd>
                <dd id="MVOnlyGATrMLP.register_load_state_dict_post_hook" class="function">register_load_state_dict_post_hook</dd>
                <dd id="MVOnlyGATrMLP.load_state_dict" class="function">load_state_dict</dd>
                <dd id="MVOnlyGATrMLP.parameters" class="function">parameters</dd>
                <dd id="MVOnlyGATrMLP.named_parameters" class="function">named_parameters</dd>
                <dd id="MVOnlyGATrMLP.buffers" class="function">buffers</dd>
                <dd id="MVOnlyGATrMLP.named_buffers" class="function">named_buffers</dd>
                <dd id="MVOnlyGATrMLP.children" class="function">children</dd>
                <dd id="MVOnlyGATrMLP.named_children" class="function">named_children</dd>
                <dd id="MVOnlyGATrMLP.modules" class="function">modules</dd>
                <dd id="MVOnlyGATrMLP.named_modules" class="function">named_modules</dd>
                <dd id="MVOnlyGATrMLP.train" class="function">train</dd>
                <dd id="MVOnlyGATrMLP.eval" class="function">eval</dd>
                <dd id="MVOnlyGATrMLP.requires_grad_" class="function">requires_grad_</dd>
                <dd id="MVOnlyGATrMLP.zero_grad" class="function">zero_grad</dd>
                <dd id="MVOnlyGATrMLP.share_memory" class="function">share_memory</dd>
                <dd id="MVOnlyGATrMLP.extra_repr" class="function">extra_repr</dd>
                <dd id="MVOnlyGATrMLP.compile" class="function">compile</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="MVOnlyGATrAttention">
                            <input id="MVOnlyGATrAttention-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">

    <span class="def">class</span>
    <span class="name">MVOnlyGATrAttention</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="MVOnlyGATrAttention-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrAttention"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrAttention-233"><a href="#MVOnlyGATrAttention-233"><span class="linenos">233</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="MVOnlyGATrAttention-234"><a href="#MVOnlyGATrAttention-234"><span class="linenos">234</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Geometric attention block without scaler channels.</span>
</span><span id="MVOnlyGATrAttention-235"><a href="#MVOnlyGATrAttention-235"><span class="linenos">235</span></a>
</span><span id="MVOnlyGATrAttention-236"><a href="#MVOnlyGATrAttention-236"><span class="linenos">236</span></a><span class="sd">    The GATr attention calculation is slightly different from the original</span>
</span><span id="MVOnlyGATrAttention-237"><a href="#MVOnlyGATrAttention-237"><span class="linenos">237</span></a><span class="sd">    transformers implementation in that each head has the sample number of</span>
</span><span id="MVOnlyGATrAttention-238"><a href="#MVOnlyGATrAttention-238"><span class="linenos">238</span></a><span class="sd">    channels as the input tensor, instead of dividing into smaller chunks.</span>
</span><span id="MVOnlyGATrAttention-239"><a href="#MVOnlyGATrAttention-239"><span class="linenos">239</span></a><span class="sd">    In this case, the final output linear transformation maps from</span>
</span><span id="MVOnlyGATrAttention-240"><a href="#MVOnlyGATrAttention-240"><span class="linenos">240</span></a><span class="sd">    ``size_channels_hidden * attn_num_heads`` to ``size_channels_hidden``.</span>
</span><span id="MVOnlyGATrAttention-241"><a href="#MVOnlyGATrAttention-241"><span class="linenos">241</span></a>
</span><span id="MVOnlyGATrAttention-242"><a href="#MVOnlyGATrAttention-242"><span class="linenos">242</span></a><span class="sd">    One additional note here is that the ``attn_mix`` parameter is a dictionary</span>
</span><span id="MVOnlyGATrAttention-243"><a href="#MVOnlyGATrAttention-243"><span class="linenos">243</span></a><span class="sd">    of learnable weighting parameter **LOGITS** for each attention kind.</span>
</span><span id="MVOnlyGATrAttention-244"><a href="#MVOnlyGATrAttention-244"><span class="linenos">244</span></a><span class="sd">    They will be exponentiated before being used in the attention calculation.</span>
</span><span id="MVOnlyGATrAttention-245"><a href="#MVOnlyGATrAttention-245"><span class="linenos">245</span></a>
</span><span id="MVOnlyGATrAttention-246"><a href="#MVOnlyGATrAttention-246"><span class="linenos">246</span></a><span class="sd">    Parameters</span>
</span><span id="MVOnlyGATrAttention-247"><a href="#MVOnlyGATrAttention-247"><span class="linenos">247</span></a><span class="sd">    ----------</span>
</span><span id="MVOnlyGATrAttention-248"><a href="#MVOnlyGATrAttention-248"><span class="linenos">248</span></a><span class="sd">    config : MVOnlyGATrConfig</span>
</span><span id="MVOnlyGATrAttention-249"><a href="#MVOnlyGATrAttention-249"><span class="linenos">249</span></a><span class="sd">        Configuration object for the model. See ``MVOnlyGATrConfig`` for more details.</span>
</span><span id="MVOnlyGATrAttention-250"><a href="#MVOnlyGATrAttention-250"><span class="linenos">250</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrAttention-251"><a href="#MVOnlyGATrAttention-251"><span class="linenos">251</span></a>
</span><span id="MVOnlyGATrAttention-252"><a href="#MVOnlyGATrAttention-252"><span class="linenos">252</span></a>    <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span>
</span><span id="MVOnlyGATrAttention-253"><a href="#MVOnlyGATrAttention-253"><span class="linenos">253</span></a>    <span class="n">layer_norm</span><span class="p">:</span> <span class="n">EquiRMSNorm</span>
</span><span id="MVOnlyGATrAttention-254"><a href="#MVOnlyGATrAttention-254"><span class="linenos">254</span></a>    <span class="n">attn_mix</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</span><span id="MVOnlyGATrAttention-255"><a href="#MVOnlyGATrAttention-255"><span class="linenos">255</span></a>    <span class="n">proj_qkv</span><span class="p">:</span> <span class="n">EquiLinear</span>
</span><span id="MVOnlyGATrAttention-256"><a href="#MVOnlyGATrAttention-256"><span class="linenos">256</span></a>
</span><span id="MVOnlyGATrAttention-257"><a href="#MVOnlyGATrAttention-257"><span class="linenos">257</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrAttention-258"><a href="#MVOnlyGATrAttention-258"><span class="linenos">258</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MVOnlyGATrAttention-259"><a href="#MVOnlyGATrAttention-259"><span class="linenos">259</span></a>
</span><span id="MVOnlyGATrAttention-260"><a href="#MVOnlyGATrAttention-260"><span class="linenos">260</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MVOnlyGATrAttention-261"><a href="#MVOnlyGATrAttention-261"><span class="linenos">261</span></a>
</span><span id="MVOnlyGATrAttention-262"><a href="#MVOnlyGATrAttention-262"><span class="linenos">262</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">EquiRMSNorm</span><span class="p">(</span>
</span><span id="MVOnlyGATrAttention-263"><a href="#MVOnlyGATrAttention-263"><span class="linenos">263</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-264"><a href="#MVOnlyGATrAttention-264"><span class="linenos">264</span></a>            <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_eps</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-265"><a href="#MVOnlyGATrAttention-265"><span class="linenos">265</span></a>            <span class="n">channelwise_rescale</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_channelwise_rescale</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-266"><a href="#MVOnlyGATrAttention-266"><span class="linenos">266</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrAttention-267"><a href="#MVOnlyGATrAttention-267"><span class="linenos">267</span></a>
</span><span id="MVOnlyGATrAttention-268"><a href="#MVOnlyGATrAttention-268"><span class="linenos">268</span></a>        <span class="c1"># The two dummy dimensions are for the sequence length</span>
</span><span id="MVOnlyGATrAttention-269"><a href="#MVOnlyGATrAttention-269"><span class="linenos">269</span></a>        <span class="c1"># and blade dimension, respectively.</span>
</span><span id="MVOnlyGATrAttention-270"><a href="#MVOnlyGATrAttention-270"><span class="linenos">270</span></a>        <span class="n">attn_mix_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="MVOnlyGATrAttention-271"><a href="#MVOnlyGATrAttention-271"><span class="linenos">271</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attn_mix</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="MVOnlyGATrAttention-272"><a href="#MVOnlyGATrAttention-272"><span class="linenos">272</span></a>        <span class="k">for</span> <span class="n">kind</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_kinds</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
</span><span id="MVOnlyGATrAttention-273"><a href="#MVOnlyGATrAttention-273"><span class="linenos">273</span></a>            <span class="n">param</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">attn_mix_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</span><span id="MVOnlyGATrAttention-274"><a href="#MVOnlyGATrAttention-274"><span class="linenos">274</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attn_mix</span><span class="p">[</span><span class="n">kind</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
</span><span id="MVOnlyGATrAttention-275"><a href="#MVOnlyGATrAttention-275"><span class="linenos">275</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;attn_mix_</span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
</span><span id="MVOnlyGATrAttention-276"><a href="#MVOnlyGATrAttention-276"><span class="linenos">276</span></a>
</span><span id="MVOnlyGATrAttention-277"><a href="#MVOnlyGATrAttention-277"><span class="linenos">277</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_qkv</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="MVOnlyGATrAttention-278"><a href="#MVOnlyGATrAttention-278"><span class="linenos">278</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-279"><a href="#MVOnlyGATrAttention-279"><span class="linenos">279</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-280"><a href="#MVOnlyGATrAttention-280"><span class="linenos">280</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrAttention-281"><a href="#MVOnlyGATrAttention-281"><span class="linenos">281</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="MVOnlyGATrAttention-282"><a href="#MVOnlyGATrAttention-282"><span class="linenos">282</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-283"><a href="#MVOnlyGATrAttention-283"><span class="linenos">283</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-284"><a href="#MVOnlyGATrAttention-284"><span class="linenos">284</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrAttention-285"><a href="#MVOnlyGATrAttention-285"><span class="linenos">285</span></a>
</span><span id="MVOnlyGATrAttention-286"><a href="#MVOnlyGATrAttention-286"><span class="linenos">286</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="MVOnlyGATrAttention-287"><a href="#MVOnlyGATrAttention-287"><span class="linenos">287</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MVOnlyGATrAttention-288"><a href="#MVOnlyGATrAttention-288"><span class="linenos">288</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MVOnlyGATrAttention-289"><a href="#MVOnlyGATrAttention-289"><span class="linenos">289</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass of the geometric attention block.</span>
</span><span id="MVOnlyGATrAttention-290"><a href="#MVOnlyGATrAttention-290"><span class="linenos">290</span></a>
</span><span id="MVOnlyGATrAttention-291"><a href="#MVOnlyGATrAttention-291"><span class="linenos">291</span></a><span class="sd">        Parameters</span>
</span><span id="MVOnlyGATrAttention-292"><a href="#MVOnlyGATrAttention-292"><span class="linenos">292</span></a><span class="sd">        ----------</span>
</span><span id="MVOnlyGATrAttention-293"><a href="#MVOnlyGATrAttention-293"><span class="linenos">293</span></a><span class="sd">        x : torch.Tensor</span>
</span><span id="MVOnlyGATrAttention-294"><a href="#MVOnlyGATrAttention-294"><span class="linenos">294</span></a><span class="sd">            Batch of input hidden multi-vector representation tensor.</span>
</span><span id="MVOnlyGATrAttention-295"><a href="#MVOnlyGATrAttention-295"><span class="linenos">295</span></a><span class="sd">        attn_mask : torch.Tensor, optional</span>
</span><span id="MVOnlyGATrAttention-296"><a href="#MVOnlyGATrAttention-296"><span class="linenos">296</span></a><span class="sd">            Attention mask tensor for the attention operation. Usually</span>
</span><span id="MVOnlyGATrAttention-297"><a href="#MVOnlyGATrAttention-297"><span class="linenos">297</span></a><span class="sd">            used if any specific attention constraints are needed within</span>
</span><span id="MVOnlyGATrAttention-298"><a href="#MVOnlyGATrAttention-298"><span class="linenos">298</span></a><span class="sd">            a single sequence, such as padding mask or for discriminating</span>
</span><span id="MVOnlyGATrAttention-299"><a href="#MVOnlyGATrAttention-299"><span class="linenos">299</span></a><span class="sd">            different subsequences.</span>
</span><span id="MVOnlyGATrAttention-300"><a href="#MVOnlyGATrAttention-300"><span class="linenos">300</span></a>
</span><span id="MVOnlyGATrAttention-301"><a href="#MVOnlyGATrAttention-301"><span class="linenos">301</span></a><span class="sd">        Returns</span>
</span><span id="MVOnlyGATrAttention-302"><a href="#MVOnlyGATrAttention-302"><span class="linenos">302</span></a><span class="sd">        -------</span>
</span><span id="MVOnlyGATrAttention-303"><a href="#MVOnlyGATrAttention-303"><span class="linenos">303</span></a><span class="sd">        torch.Tensor</span>
</span><span id="MVOnlyGATrAttention-304"><a href="#MVOnlyGATrAttention-304"><span class="linenos">304</span></a><span class="sd">            Batch of output hidden multi-vector representation tensor of the</span>
</span><span id="MVOnlyGATrAttention-305"><a href="#MVOnlyGATrAttention-305"><span class="linenos">305</span></a><span class="sd">            same number of hidden channels.</span>
</span><span id="MVOnlyGATrAttention-306"><a href="#MVOnlyGATrAttention-306"><span class="linenos">306</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrAttention-307"><a href="#MVOnlyGATrAttention-307"><span class="linenos">307</span></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="MVOnlyGATrAttention-308"><a href="#MVOnlyGATrAttention-308"><span class="linenos">308</span></a>
</span><span id="MVOnlyGATrAttention-309"><a href="#MVOnlyGATrAttention-309"><span class="linenos">309</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MVOnlyGATrAttention-310"><a href="#MVOnlyGATrAttention-310"><span class="linenos">310</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
</span><span id="MVOnlyGATrAttention-311"><a href="#MVOnlyGATrAttention-311"><span class="linenos">311</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">proj_qkv</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
</span><span id="MVOnlyGATrAttention-312"><a href="#MVOnlyGATrAttention-312"><span class="linenos">312</span></a>            <span class="s2">&quot;b t (qkv h c) k -&gt; qkv b h t c k&quot;</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-313"><a href="#MVOnlyGATrAttention-313"><span class="linenos">313</span></a>            <span class="n">qkv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-314"><a href="#MVOnlyGATrAttention-314"><span class="linenos">314</span></a>            <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-315"><a href="#MVOnlyGATrAttention-315"><span class="linenos">315</span></a>            <span class="n">c</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-316"><a href="#MVOnlyGATrAttention-316"><span class="linenos">316</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrAttention-317"><a href="#MVOnlyGATrAttention-317"><span class="linenos">317</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">equi_geometric_attention</span><span class="p">(</span>
</span><span id="MVOnlyGATrAttention-318"><a href="#MVOnlyGATrAttention-318"><span class="linenos">318</span></a>            <span class="n">q</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-319"><a href="#MVOnlyGATrAttention-319"><span class="linenos">319</span></a>            <span class="n">k</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-320"><a href="#MVOnlyGATrAttention-320"><span class="linenos">320</span></a>            <span class="n">v</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-321"><a href="#MVOnlyGATrAttention-321"><span class="linenos">321</span></a>            <span class="n">kinds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_kinds</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-322"><a href="#MVOnlyGATrAttention-322"><span class="linenos">322</span></a>            <span class="n">weight</span><span class="o">=</span><span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_mix</span><span class="o">.</span><span class="n">values</span><span class="p">()],</span>
</span><span id="MVOnlyGATrAttention-323"><a href="#MVOnlyGATrAttention-323"><span class="linenos">323</span></a>            <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-324"><a href="#MVOnlyGATrAttention-324"><span class="linenos">324</span></a>            <span class="n">is_causal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_is_causal</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-325"><a href="#MVOnlyGATrAttention-325"><span class="linenos">325</span></a>            <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_dropout_p</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-326"><a href="#MVOnlyGATrAttention-326"><span class="linenos">326</span></a>            <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_scale</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention-327"><a href="#MVOnlyGATrAttention-327"><span class="linenos">327</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrAttention-328"><a href="#MVOnlyGATrAttention-328"><span class="linenos">328</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;b h t c k -&gt; b t (h c) k&quot;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span><span class="p">)</span>
</span><span id="MVOnlyGATrAttention-329"><a href="#MVOnlyGATrAttention-329"><span class="linenos">329</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MVOnlyGATrAttention-330"><a href="#MVOnlyGATrAttention-330"><span class="linenos">330</span></a>
</span><span id="MVOnlyGATrAttention-331"><a href="#MVOnlyGATrAttention-331"><span class="linenos">331</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">residual</span>
</span></pre></div>


            <div class="docstring"><p>Geometric attention block without scaler channels.</p>

<p>The GATr attention calculation is slightly different from the original
transformers implementation in that each head has the sample number of
channels as the input tensor, instead of dividing into smaller chunks.
In this case, the final output linear transformation maps from
<code>size_channels_hidden * attn_num_heads</code> to <code>size_channels_hidden</code>.</p>

<p>One additional note here is that the <code><a href="#MVOnlyGATrAttention.attn_mix">attn_mix</a></code> parameter is a dictionary
of learnable weighting parameter <strong>LOGITS</strong> for each attention kind.
They will be exponentiated before being used in the attention calculation.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>config</strong> (MVOnlyGATrConfig):
Configuration object for the model. See <code><a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></code> for more details.</li>
</ul>
</div>


                            <div id="MVOnlyGATrAttention.__init__" class="classattr">
                                        <input id="MVOnlyGATrAttention.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">

        <span class="name">MVOnlyGATrAttention</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">config</span><span class="p">:</span> <span class="n"><a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></span></span>)</span>

                <label class="view-source-button" for="MVOnlyGATrAttention.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrAttention.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrAttention.__init__-257"><a href="#MVOnlyGATrAttention.__init__-257"><span class="linenos">257</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrAttention.__init__-258"><a href="#MVOnlyGATrAttention.__init__-258"><span class="linenos">258</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MVOnlyGATrAttention.__init__-259"><a href="#MVOnlyGATrAttention.__init__-259"><span class="linenos">259</span></a>
</span><span id="MVOnlyGATrAttention.__init__-260"><a href="#MVOnlyGATrAttention.__init__-260"><span class="linenos">260</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MVOnlyGATrAttention.__init__-261"><a href="#MVOnlyGATrAttention.__init__-261"><span class="linenos">261</span></a>
</span><span id="MVOnlyGATrAttention.__init__-262"><a href="#MVOnlyGATrAttention.__init__-262"><span class="linenos">262</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">EquiRMSNorm</span><span class="p">(</span>
</span><span id="MVOnlyGATrAttention.__init__-263"><a href="#MVOnlyGATrAttention.__init__-263"><span class="linenos">263</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.__init__-264"><a href="#MVOnlyGATrAttention.__init__-264"><span class="linenos">264</span></a>            <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_eps</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.__init__-265"><a href="#MVOnlyGATrAttention.__init__-265"><span class="linenos">265</span></a>            <span class="n">channelwise_rescale</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_channelwise_rescale</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.__init__-266"><a href="#MVOnlyGATrAttention.__init__-266"><span class="linenos">266</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrAttention.__init__-267"><a href="#MVOnlyGATrAttention.__init__-267"><span class="linenos">267</span></a>
</span><span id="MVOnlyGATrAttention.__init__-268"><a href="#MVOnlyGATrAttention.__init__-268"><span class="linenos">268</span></a>        <span class="c1"># The two dummy dimensions are for the sequence length</span>
</span><span id="MVOnlyGATrAttention.__init__-269"><a href="#MVOnlyGATrAttention.__init__-269"><span class="linenos">269</span></a>        <span class="c1"># and blade dimension, respectively.</span>
</span><span id="MVOnlyGATrAttention.__init__-270"><a href="#MVOnlyGATrAttention.__init__-270"><span class="linenos">270</span></a>        <span class="n">attn_mix_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="MVOnlyGATrAttention.__init__-271"><a href="#MVOnlyGATrAttention.__init__-271"><span class="linenos">271</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attn_mix</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="MVOnlyGATrAttention.__init__-272"><a href="#MVOnlyGATrAttention.__init__-272"><span class="linenos">272</span></a>        <span class="k">for</span> <span class="n">kind</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_kinds</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
</span><span id="MVOnlyGATrAttention.__init__-273"><a href="#MVOnlyGATrAttention.__init__-273"><span class="linenos">273</span></a>            <span class="n">param</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">attn_mix_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</span><span id="MVOnlyGATrAttention.__init__-274"><a href="#MVOnlyGATrAttention.__init__-274"><span class="linenos">274</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attn_mix</span><span class="p">[</span><span class="n">kind</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
</span><span id="MVOnlyGATrAttention.__init__-275"><a href="#MVOnlyGATrAttention.__init__-275"><span class="linenos">275</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;attn_mix_</span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
</span><span id="MVOnlyGATrAttention.__init__-276"><a href="#MVOnlyGATrAttention.__init__-276"><span class="linenos">276</span></a>
</span><span id="MVOnlyGATrAttention.__init__-277"><a href="#MVOnlyGATrAttention.__init__-277"><span class="linenos">277</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_qkv</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="MVOnlyGATrAttention.__init__-278"><a href="#MVOnlyGATrAttention.__init__-278"><span class="linenos">278</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.__init__-279"><a href="#MVOnlyGATrAttention.__init__-279"><span class="linenos">279</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.__init__-280"><a href="#MVOnlyGATrAttention.__init__-280"><span class="linenos">280</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrAttention.__init__-281"><a href="#MVOnlyGATrAttention.__init__-281"><span class="linenos">281</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span>
</span><span id="MVOnlyGATrAttention.__init__-282"><a href="#MVOnlyGATrAttention.__init__-282"><span class="linenos">282</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.__init__-283"><a href="#MVOnlyGATrAttention.__init__-283"><span class="linenos">283</span></a>            <span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.__init__-284"><a href="#MVOnlyGATrAttention.__init__-284"><span class="linenos">284</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="MVOnlyGATrAttention.config" class="classattr">
                                <div class="attr variable">
            <span class="name">config</span><span class="annotation">: <a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrAttention.config"></a>



                            </div>
                            <div id="MVOnlyGATrAttention.layer_norm" class="classattr">
                                <div class="attr variable">
            <span class="name">layer_norm</span><span class="annotation">: <a href="../nn/modules.html#EquiRMSNorm">ezgatr.nn.modules.EquiRMSNorm</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrAttention.layer_norm"></a>



                            </div>
                            <div id="MVOnlyGATrAttention.attn_mix" class="classattr">
                                <div class="attr variable">
            <span class="name">attn_mix</span><span class="annotation">: dict[str, torch.Tensor]</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrAttention.attn_mix"></a>



                            </div>
                            <div id="MVOnlyGATrAttention.proj_qkv" class="classattr">
                                <div class="attr variable">
            <span class="name">proj_qkv</span><span class="annotation">: <a href="../nn/modules.html#EquiLinear">ezgatr.nn.modules.EquiLinear</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrAttention.proj_qkv"></a>



                            </div>
                            <div id="MVOnlyGATrAttention.proj_out" class="classattr">
                                <div class="attr variable">
            <span class="name">proj_out</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrAttention.proj_out"></a>



                            </div>
                            <div id="MVOnlyGATrAttention.forward" class="classattr">
                                        <input id="MVOnlyGATrAttention.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">

        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="MVOnlyGATrAttention.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrAttention.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrAttention.forward-286"><a href="#MVOnlyGATrAttention.forward-286"><span class="linenos">286</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="MVOnlyGATrAttention.forward-287"><a href="#MVOnlyGATrAttention.forward-287"><span class="linenos">287</span></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MVOnlyGATrAttention.forward-288"><a href="#MVOnlyGATrAttention.forward-288"><span class="linenos">288</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MVOnlyGATrAttention.forward-289"><a href="#MVOnlyGATrAttention.forward-289"><span class="linenos">289</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass of the geometric attention block.</span>
</span><span id="MVOnlyGATrAttention.forward-290"><a href="#MVOnlyGATrAttention.forward-290"><span class="linenos">290</span></a>
</span><span id="MVOnlyGATrAttention.forward-291"><a href="#MVOnlyGATrAttention.forward-291"><span class="linenos">291</span></a><span class="sd">        Parameters</span>
</span><span id="MVOnlyGATrAttention.forward-292"><a href="#MVOnlyGATrAttention.forward-292"><span class="linenos">292</span></a><span class="sd">        ----------</span>
</span><span id="MVOnlyGATrAttention.forward-293"><a href="#MVOnlyGATrAttention.forward-293"><span class="linenos">293</span></a><span class="sd">        x : torch.Tensor</span>
</span><span id="MVOnlyGATrAttention.forward-294"><a href="#MVOnlyGATrAttention.forward-294"><span class="linenos">294</span></a><span class="sd">            Batch of input hidden multi-vector representation tensor.</span>
</span><span id="MVOnlyGATrAttention.forward-295"><a href="#MVOnlyGATrAttention.forward-295"><span class="linenos">295</span></a><span class="sd">        attn_mask : torch.Tensor, optional</span>
</span><span id="MVOnlyGATrAttention.forward-296"><a href="#MVOnlyGATrAttention.forward-296"><span class="linenos">296</span></a><span class="sd">            Attention mask tensor for the attention operation. Usually</span>
</span><span id="MVOnlyGATrAttention.forward-297"><a href="#MVOnlyGATrAttention.forward-297"><span class="linenos">297</span></a><span class="sd">            used if any specific attention constraints are needed within</span>
</span><span id="MVOnlyGATrAttention.forward-298"><a href="#MVOnlyGATrAttention.forward-298"><span class="linenos">298</span></a><span class="sd">            a single sequence, such as padding mask or for discriminating</span>
</span><span id="MVOnlyGATrAttention.forward-299"><a href="#MVOnlyGATrAttention.forward-299"><span class="linenos">299</span></a><span class="sd">            different subsequences.</span>
</span><span id="MVOnlyGATrAttention.forward-300"><a href="#MVOnlyGATrAttention.forward-300"><span class="linenos">300</span></a>
</span><span id="MVOnlyGATrAttention.forward-301"><a href="#MVOnlyGATrAttention.forward-301"><span class="linenos">301</span></a><span class="sd">        Returns</span>
</span><span id="MVOnlyGATrAttention.forward-302"><a href="#MVOnlyGATrAttention.forward-302"><span class="linenos">302</span></a><span class="sd">        -------</span>
</span><span id="MVOnlyGATrAttention.forward-303"><a href="#MVOnlyGATrAttention.forward-303"><span class="linenos">303</span></a><span class="sd">        torch.Tensor</span>
</span><span id="MVOnlyGATrAttention.forward-304"><a href="#MVOnlyGATrAttention.forward-304"><span class="linenos">304</span></a><span class="sd">            Batch of output hidden multi-vector representation tensor of the</span>
</span><span id="MVOnlyGATrAttention.forward-305"><a href="#MVOnlyGATrAttention.forward-305"><span class="linenos">305</span></a><span class="sd">            same number of hidden channels.</span>
</span><span id="MVOnlyGATrAttention.forward-306"><a href="#MVOnlyGATrAttention.forward-306"><span class="linenos">306</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrAttention.forward-307"><a href="#MVOnlyGATrAttention.forward-307"><span class="linenos">307</span></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="MVOnlyGATrAttention.forward-308"><a href="#MVOnlyGATrAttention.forward-308"><span class="linenos">308</span></a>
</span><span id="MVOnlyGATrAttention.forward-309"><a href="#MVOnlyGATrAttention.forward-309"><span class="linenos">309</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MVOnlyGATrAttention.forward-310"><a href="#MVOnlyGATrAttention.forward-310"><span class="linenos">310</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
</span><span id="MVOnlyGATrAttention.forward-311"><a href="#MVOnlyGATrAttention.forward-311"><span class="linenos">311</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">proj_qkv</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
</span><span id="MVOnlyGATrAttention.forward-312"><a href="#MVOnlyGATrAttention.forward-312"><span class="linenos">312</span></a>            <span class="s2">&quot;b t (qkv h c) k -&gt; qkv b h t c k&quot;</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.forward-313"><a href="#MVOnlyGATrAttention.forward-313"><span class="linenos">313</span></a>            <span class="n">qkv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.forward-314"><a href="#MVOnlyGATrAttention.forward-314"><span class="linenos">314</span></a>            <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.forward-315"><a href="#MVOnlyGATrAttention.forward-315"><span class="linenos">315</span></a>            <span class="n">c</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.forward-316"><a href="#MVOnlyGATrAttention.forward-316"><span class="linenos">316</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrAttention.forward-317"><a href="#MVOnlyGATrAttention.forward-317"><span class="linenos">317</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">equi_geometric_attention</span><span class="p">(</span>
</span><span id="MVOnlyGATrAttention.forward-318"><a href="#MVOnlyGATrAttention.forward-318"><span class="linenos">318</span></a>            <span class="n">q</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.forward-319"><a href="#MVOnlyGATrAttention.forward-319"><span class="linenos">319</span></a>            <span class="n">k</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.forward-320"><a href="#MVOnlyGATrAttention.forward-320"><span class="linenos">320</span></a>            <span class="n">v</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.forward-321"><a href="#MVOnlyGATrAttention.forward-321"><span class="linenos">321</span></a>            <span class="n">kinds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_kinds</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.forward-322"><a href="#MVOnlyGATrAttention.forward-322"><span class="linenos">322</span></a>            <span class="n">weight</span><span class="o">=</span><span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_mix</span><span class="o">.</span><span class="n">values</span><span class="p">()],</span>
</span><span id="MVOnlyGATrAttention.forward-323"><a href="#MVOnlyGATrAttention.forward-323"><span class="linenos">323</span></a>            <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.forward-324"><a href="#MVOnlyGATrAttention.forward-324"><span class="linenos">324</span></a>            <span class="n">is_causal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_is_causal</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.forward-325"><a href="#MVOnlyGATrAttention.forward-325"><span class="linenos">325</span></a>            <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_dropout_p</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.forward-326"><a href="#MVOnlyGATrAttention.forward-326"><span class="linenos">326</span></a>            <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_scale</span><span class="p">,</span>
</span><span id="MVOnlyGATrAttention.forward-327"><a href="#MVOnlyGATrAttention.forward-327"><span class="linenos">327</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrAttention.forward-328"><a href="#MVOnlyGATrAttention.forward-328"><span class="linenos">328</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;b h t c k -&gt; b t (h c) k&quot;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_num_heads</span><span class="p">)</span>
</span><span id="MVOnlyGATrAttention.forward-329"><a href="#MVOnlyGATrAttention.forward-329"><span class="linenos">329</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MVOnlyGATrAttention.forward-330"><a href="#MVOnlyGATrAttention.forward-330"><span class="linenos">330</span></a>
</span><span id="MVOnlyGATrAttention.forward-331"><a href="#MVOnlyGATrAttention.forward-331"><span class="linenos">331</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">residual</span>
</span></pre></div>


            <div class="docstring"><p>Forward pass of the geometric attention block.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>x</strong> (torch.Tensor):
Batch of input hidden multi-vector representation tensor.</li>
<li><strong>attn_mask</strong> (torch.Tensor, optional):
Attention mask tensor for the attention operation. Usually
used if any specific attention constraints are needed within
a single sequence, such as padding mask or for discriminating
different subsequences.</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>torch.Tensor</strong>: Batch of output hidden multi-vector representation tensor of the
same number of hidden channels.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="MVOnlyGATrAttention.dump_patches" class="variable">dump_patches</dd>
                <dd id="MVOnlyGATrAttention.training" class="variable">training</dd>
                <dd id="MVOnlyGATrAttention.call_super_init" class="variable">call_super_init</dd>
                <dd id="MVOnlyGATrAttention.register_buffer" class="function">register_buffer</dd>
                <dd id="MVOnlyGATrAttention.register_parameter" class="function">register_parameter</dd>
                <dd id="MVOnlyGATrAttention.add_module" class="function">add_module</dd>
                <dd id="MVOnlyGATrAttention.register_module" class="function">register_module</dd>
                <dd id="MVOnlyGATrAttention.get_submodule" class="function">get_submodule</dd>
                <dd id="MVOnlyGATrAttention.get_parameter" class="function">get_parameter</dd>
                <dd id="MVOnlyGATrAttention.get_buffer" class="function">get_buffer</dd>
                <dd id="MVOnlyGATrAttention.get_extra_state" class="function">get_extra_state</dd>
                <dd id="MVOnlyGATrAttention.set_extra_state" class="function">set_extra_state</dd>
                <dd id="MVOnlyGATrAttention.apply" class="function">apply</dd>
                <dd id="MVOnlyGATrAttention.cuda" class="function">cuda</dd>
                <dd id="MVOnlyGATrAttention.ipu" class="function">ipu</dd>
                <dd id="MVOnlyGATrAttention.xpu" class="function">xpu</dd>
                <dd id="MVOnlyGATrAttention.cpu" class="function">cpu</dd>
                <dd id="MVOnlyGATrAttention.type" class="function">type</dd>
                <dd id="MVOnlyGATrAttention.float" class="function">float</dd>
                <dd id="MVOnlyGATrAttention.double" class="function">double</dd>
                <dd id="MVOnlyGATrAttention.half" class="function">half</dd>
                <dd id="MVOnlyGATrAttention.bfloat16" class="function">bfloat16</dd>
                <dd id="MVOnlyGATrAttention.to_empty" class="function">to_empty</dd>
                <dd id="MVOnlyGATrAttention.to" class="function">to</dd>
                <dd id="MVOnlyGATrAttention.register_full_backward_pre_hook" class="function">register_full_backward_pre_hook</dd>
                <dd id="MVOnlyGATrAttention.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="MVOnlyGATrAttention.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="MVOnlyGATrAttention.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="MVOnlyGATrAttention.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="MVOnlyGATrAttention.register_state_dict_pre_hook" class="function">register_state_dict_pre_hook</dd>
                <dd id="MVOnlyGATrAttention.state_dict" class="function">state_dict</dd>
                <dd id="MVOnlyGATrAttention.register_load_state_dict_post_hook" class="function">register_load_state_dict_post_hook</dd>
                <dd id="MVOnlyGATrAttention.load_state_dict" class="function">load_state_dict</dd>
                <dd id="MVOnlyGATrAttention.parameters" class="function">parameters</dd>
                <dd id="MVOnlyGATrAttention.named_parameters" class="function">named_parameters</dd>
                <dd id="MVOnlyGATrAttention.buffers" class="function">buffers</dd>
                <dd id="MVOnlyGATrAttention.named_buffers" class="function">named_buffers</dd>
                <dd id="MVOnlyGATrAttention.children" class="function">children</dd>
                <dd id="MVOnlyGATrAttention.named_children" class="function">named_children</dd>
                <dd id="MVOnlyGATrAttention.modules" class="function">modules</dd>
                <dd id="MVOnlyGATrAttention.named_modules" class="function">named_modules</dd>
                <dd id="MVOnlyGATrAttention.train" class="function">train</dd>
                <dd id="MVOnlyGATrAttention.eval" class="function">eval</dd>
                <dd id="MVOnlyGATrAttention.requires_grad_" class="function">requires_grad_</dd>
                <dd id="MVOnlyGATrAttention.zero_grad" class="function">zero_grad</dd>
                <dd id="MVOnlyGATrAttention.share_memory" class="function">share_memory</dd>
                <dd id="MVOnlyGATrAttention.extra_repr" class="function">extra_repr</dd>
                <dd id="MVOnlyGATrAttention.compile" class="function">compile</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="MVOnlyGATrBlock">
                            <input id="MVOnlyGATrBlock-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">

    <span class="def">class</span>
    <span class="name">MVOnlyGATrBlock</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="MVOnlyGATrBlock-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrBlock"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrBlock-334"><a href="#MVOnlyGATrBlock-334"><span class="linenos">334</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="MVOnlyGATrBlock-335"><a href="#MVOnlyGATrBlock-335"><span class="linenos">335</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;GATr block without scaler channels.</span>
</span><span id="MVOnlyGATrBlock-336"><a href="#MVOnlyGATrBlock-336"><span class="linenos">336</span></a>
</span><span id="MVOnlyGATrBlock-337"><a href="#MVOnlyGATrBlock-337"><span class="linenos">337</span></a><span class="sd">    Parameters</span>
</span><span id="MVOnlyGATrBlock-338"><a href="#MVOnlyGATrBlock-338"><span class="linenos">338</span></a><span class="sd">    ----------</span>
</span><span id="MVOnlyGATrBlock-339"><a href="#MVOnlyGATrBlock-339"><span class="linenos">339</span></a><span class="sd">    config : MVOnlyGATrConfig</span>
</span><span id="MVOnlyGATrBlock-340"><a href="#MVOnlyGATrBlock-340"><span class="linenos">340</span></a><span class="sd">        Configuration object for the model. See ``MVOnlyGATrConfig`` for more details.</span>
</span><span id="MVOnlyGATrBlock-341"><a href="#MVOnlyGATrBlock-341"><span class="linenos">341</span></a><span class="sd">    layer_id : int</span>
</span><span id="MVOnlyGATrBlock-342"><a href="#MVOnlyGATrBlock-342"><span class="linenos">342</span></a><span class="sd">        Index of the current block in the network.</span>
</span><span id="MVOnlyGATrBlock-343"><a href="#MVOnlyGATrBlock-343"><span class="linenos">343</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrBlock-344"><a href="#MVOnlyGATrBlock-344"><span class="linenos">344</span></a>
</span><span id="MVOnlyGATrBlock-345"><a href="#MVOnlyGATrBlock-345"><span class="linenos">345</span></a>    <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span>
</span><span id="MVOnlyGATrBlock-346"><a href="#MVOnlyGATrBlock-346"><span class="linenos">346</span></a>    <span class="n">layer_id</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="MVOnlyGATrBlock-347"><a href="#MVOnlyGATrBlock-347"><span class="linenos">347</span></a>    <span class="n">mlp</span><span class="p">:</span> <span class="n">MVOnlyGATrMLP</span>
</span><span id="MVOnlyGATrBlock-348"><a href="#MVOnlyGATrBlock-348"><span class="linenos">348</span></a>    <span class="n">attn</span><span class="p">:</span> <span class="n">MVOnlyGATrAttention</span>
</span><span id="MVOnlyGATrBlock-349"><a href="#MVOnlyGATrBlock-349"><span class="linenos">349</span></a>
</span><span id="MVOnlyGATrBlock-350"><a href="#MVOnlyGATrBlock-350"><span class="linenos">350</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrBlock-351"><a href="#MVOnlyGATrBlock-351"><span class="linenos">351</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MVOnlyGATrBlock-352"><a href="#MVOnlyGATrBlock-352"><span class="linenos">352</span></a>
</span><span id="MVOnlyGATrBlock-353"><a href="#MVOnlyGATrBlock-353"><span class="linenos">353</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MVOnlyGATrBlock-354"><a href="#MVOnlyGATrBlock-354"><span class="linenos">354</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_id</span> <span class="o">=</span> <span class="n">layer_id</span>
</span><span id="MVOnlyGATrBlock-355"><a href="#MVOnlyGATrBlock-355"><span class="linenos">355</span></a>
</span><span id="MVOnlyGATrBlock-356"><a href="#MVOnlyGATrBlock-356"><span class="linenos">356</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MVOnlyGATrMLP</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span id="MVOnlyGATrBlock-357"><a href="#MVOnlyGATrBlock-357"><span class="linenos">357</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">MVOnlyGATrAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span id="MVOnlyGATrBlock-358"><a href="#MVOnlyGATrBlock-358"><span class="linenos">358</span></a>
</span><span id="MVOnlyGATrBlock-359"><a href="#MVOnlyGATrBlock-359"><span class="linenos">359</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="MVOnlyGATrBlock-360"><a href="#MVOnlyGATrBlock-360"><span class="linenos">360</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MVOnlyGATrBlock-361"><a href="#MVOnlyGATrBlock-361"><span class="linenos">361</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MVOnlyGATrBlock-362"><a href="#MVOnlyGATrBlock-362"><span class="linenos">362</span></a>        <span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MVOnlyGATrBlock-363"><a href="#MVOnlyGATrBlock-363"><span class="linenos">363</span></a>        <span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MVOnlyGATrBlock-364"><a href="#MVOnlyGATrBlock-364"><span class="linenos">364</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MVOnlyGATrBlock-365"><a href="#MVOnlyGATrBlock-365"><span class="linenos">365</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reference</span><span class="p">),</span> <span class="n">attn_mask</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>GATr block without scaler channels.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>config</strong> (MVOnlyGATrConfig):
Configuration object for the model. See <code><a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></code> for more details.</li>
<li><strong>layer_id</strong> (int):
Index of the current block in the network.</li>
</ul>
</div>


                            <div id="MVOnlyGATrBlock.__init__" class="classattr">
                                        <input id="MVOnlyGATrBlock.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">

        <span class="name">MVOnlyGATrBlock</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">config</span><span class="p">:</span> <span class="n"><a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></span>, </span><span class="param"><span class="n">layer_id</span><span class="p">:</span> <span class="nb">int</span></span>)</span>

                <label class="view-source-button" for="MVOnlyGATrBlock.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrBlock.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrBlock.__init__-350"><a href="#MVOnlyGATrBlock.__init__-350"><span class="linenos">350</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrBlock.__init__-351"><a href="#MVOnlyGATrBlock.__init__-351"><span class="linenos">351</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MVOnlyGATrBlock.__init__-352"><a href="#MVOnlyGATrBlock.__init__-352"><span class="linenos">352</span></a>
</span><span id="MVOnlyGATrBlock.__init__-353"><a href="#MVOnlyGATrBlock.__init__-353"><span class="linenos">353</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MVOnlyGATrBlock.__init__-354"><a href="#MVOnlyGATrBlock.__init__-354"><span class="linenos">354</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_id</span> <span class="o">=</span> <span class="n">layer_id</span>
</span><span id="MVOnlyGATrBlock.__init__-355"><a href="#MVOnlyGATrBlock.__init__-355"><span class="linenos">355</span></a>
</span><span id="MVOnlyGATrBlock.__init__-356"><a href="#MVOnlyGATrBlock.__init__-356"><span class="linenos">356</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MVOnlyGATrMLP</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span id="MVOnlyGATrBlock.__init__-357"><a href="#MVOnlyGATrBlock.__init__-357"><span class="linenos">357</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">MVOnlyGATrAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="MVOnlyGATrBlock.config" class="classattr">
                                <div class="attr variable">
            <span class="name">config</span><span class="annotation">: <a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrBlock.config"></a>



                            </div>
                            <div id="MVOnlyGATrBlock.layer_id" class="classattr">
                                <div class="attr variable">
            <span class="name">layer_id</span><span class="annotation">: int</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrBlock.layer_id"></a>



                            </div>
                            <div id="MVOnlyGATrBlock.mlp" class="classattr">
                                <div class="attr variable">
            <span class="name">mlp</span><span class="annotation">: <a href="#MVOnlyGATrMLP">MVOnlyGATrMLP</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrBlock.mlp"></a>



                            </div>
                            <div id="MVOnlyGATrBlock.attn" class="classattr">
                                <div class="attr variable">
            <span class="name">attn</span><span class="annotation">: <a href="#MVOnlyGATrAttention">MVOnlyGATrAttention</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrBlock.attn"></a>



                            </div>
                            <div id="MVOnlyGATrBlock.forward" class="classattr">
                                        <input id="MVOnlyGATrBlock.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">

        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="MVOnlyGATrBlock.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrBlock.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrBlock.forward-359"><a href="#MVOnlyGATrBlock.forward-359"><span class="linenos">359</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="MVOnlyGATrBlock.forward-360"><a href="#MVOnlyGATrBlock.forward-360"><span class="linenos">360</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MVOnlyGATrBlock.forward-361"><a href="#MVOnlyGATrBlock.forward-361"><span class="linenos">361</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MVOnlyGATrBlock.forward-362"><a href="#MVOnlyGATrBlock.forward-362"><span class="linenos">362</span></a>        <span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MVOnlyGATrBlock.forward-363"><a href="#MVOnlyGATrBlock.forward-363"><span class="linenos">363</span></a>        <span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MVOnlyGATrBlock.forward-364"><a href="#MVOnlyGATrBlock.forward-364"><span class="linenos">364</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MVOnlyGATrBlock.forward-365"><a href="#MVOnlyGATrBlock.forward-365"><span class="linenos">365</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reference</span><span class="p">),</span> <span class="n">attn_mask</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="MVOnlyGATrBlock.dump_patches" class="variable">dump_patches</dd>
                <dd id="MVOnlyGATrBlock.training" class="variable">training</dd>
                <dd id="MVOnlyGATrBlock.call_super_init" class="variable">call_super_init</dd>
                <dd id="MVOnlyGATrBlock.register_buffer" class="function">register_buffer</dd>
                <dd id="MVOnlyGATrBlock.register_parameter" class="function">register_parameter</dd>
                <dd id="MVOnlyGATrBlock.add_module" class="function">add_module</dd>
                <dd id="MVOnlyGATrBlock.register_module" class="function">register_module</dd>
                <dd id="MVOnlyGATrBlock.get_submodule" class="function">get_submodule</dd>
                <dd id="MVOnlyGATrBlock.get_parameter" class="function">get_parameter</dd>
                <dd id="MVOnlyGATrBlock.get_buffer" class="function">get_buffer</dd>
                <dd id="MVOnlyGATrBlock.get_extra_state" class="function">get_extra_state</dd>
                <dd id="MVOnlyGATrBlock.set_extra_state" class="function">set_extra_state</dd>
                <dd id="MVOnlyGATrBlock.apply" class="function">apply</dd>
                <dd id="MVOnlyGATrBlock.cuda" class="function">cuda</dd>
                <dd id="MVOnlyGATrBlock.ipu" class="function">ipu</dd>
                <dd id="MVOnlyGATrBlock.xpu" class="function">xpu</dd>
                <dd id="MVOnlyGATrBlock.cpu" class="function">cpu</dd>
                <dd id="MVOnlyGATrBlock.type" class="function">type</dd>
                <dd id="MVOnlyGATrBlock.float" class="function">float</dd>
                <dd id="MVOnlyGATrBlock.double" class="function">double</dd>
                <dd id="MVOnlyGATrBlock.half" class="function">half</dd>
                <dd id="MVOnlyGATrBlock.bfloat16" class="function">bfloat16</dd>
                <dd id="MVOnlyGATrBlock.to_empty" class="function">to_empty</dd>
                <dd id="MVOnlyGATrBlock.to" class="function">to</dd>
                <dd id="MVOnlyGATrBlock.register_full_backward_pre_hook" class="function">register_full_backward_pre_hook</dd>
                <dd id="MVOnlyGATrBlock.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="MVOnlyGATrBlock.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="MVOnlyGATrBlock.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="MVOnlyGATrBlock.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="MVOnlyGATrBlock.register_state_dict_pre_hook" class="function">register_state_dict_pre_hook</dd>
                <dd id="MVOnlyGATrBlock.state_dict" class="function">state_dict</dd>
                <dd id="MVOnlyGATrBlock.register_load_state_dict_post_hook" class="function">register_load_state_dict_post_hook</dd>
                <dd id="MVOnlyGATrBlock.load_state_dict" class="function">load_state_dict</dd>
                <dd id="MVOnlyGATrBlock.parameters" class="function">parameters</dd>
                <dd id="MVOnlyGATrBlock.named_parameters" class="function">named_parameters</dd>
                <dd id="MVOnlyGATrBlock.buffers" class="function">buffers</dd>
                <dd id="MVOnlyGATrBlock.named_buffers" class="function">named_buffers</dd>
                <dd id="MVOnlyGATrBlock.children" class="function">children</dd>
                <dd id="MVOnlyGATrBlock.named_children" class="function">named_children</dd>
                <dd id="MVOnlyGATrBlock.modules" class="function">modules</dd>
                <dd id="MVOnlyGATrBlock.named_modules" class="function">named_modules</dd>
                <dd id="MVOnlyGATrBlock.train" class="function">train</dd>
                <dd id="MVOnlyGATrBlock.eval" class="function">eval</dd>
                <dd id="MVOnlyGATrBlock.requires_grad_" class="function">requires_grad_</dd>
                <dd id="MVOnlyGATrBlock.zero_grad" class="function">zero_grad</dd>
                <dd id="MVOnlyGATrBlock.share_memory" class="function">share_memory</dd>
                <dd id="MVOnlyGATrBlock.extra_repr" class="function">extra_repr</dd>
                <dd id="MVOnlyGATrBlock.compile" class="function">compile</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="MVOnlyGATrModel">
                            <input id="MVOnlyGATrModel-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">

    <span class="def">class</span>
    <span class="name">MVOnlyGATrModel</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="MVOnlyGATrModel-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrModel"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrModel-368"><a href="#MVOnlyGATrModel-368"><span class="linenos">368</span></a><span class="k">class</span> <span class="nc">MVOnlyGATrModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="MVOnlyGATrModel-369"><a href="#MVOnlyGATrModel-369"><span class="linenos">369</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Multi-Vector only GATr model.</span>
</span><span id="MVOnlyGATrModel-370"><a href="#MVOnlyGATrModel-370"><span class="linenos">370</span></a>
</span><span id="MVOnlyGATrModel-371"><a href="#MVOnlyGATrModel-371"><span class="linenos">371</span></a><span class="sd">    Parameters</span>
</span><span id="MVOnlyGATrModel-372"><a href="#MVOnlyGATrModel-372"><span class="linenos">372</span></a><span class="sd">    ----------</span>
</span><span id="MVOnlyGATrModel-373"><a href="#MVOnlyGATrModel-373"><span class="linenos">373</span></a><span class="sd">    config : MVOnlyGATrConfig</span>
</span><span id="MVOnlyGATrModel-374"><a href="#MVOnlyGATrModel-374"><span class="linenos">374</span></a><span class="sd">        Configuration object for the model. See ``MVOnlyGATrConfig`` for more details.</span>
</span><span id="MVOnlyGATrModel-375"><a href="#MVOnlyGATrModel-375"><span class="linenos">375</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrModel-376"><a href="#MVOnlyGATrModel-376"><span class="linenos">376</span></a>
</span><span id="MVOnlyGATrModel-377"><a href="#MVOnlyGATrModel-377"><span class="linenos">377</span></a>    <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span>
</span><span id="MVOnlyGATrModel-378"><a href="#MVOnlyGATrModel-378"><span class="linenos">378</span></a>    <span class="n">embedding</span><span class="p">:</span> <span class="n">MVOnlyGATrEmbedding</span>
</span><span id="MVOnlyGATrModel-379"><a href="#MVOnlyGATrModel-379"><span class="linenos">379</span></a>    <span class="n">blocks</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span>
</span><span id="MVOnlyGATrModel-380"><a href="#MVOnlyGATrModel-380"><span class="linenos">380</span></a>    <span class="n">head</span><span class="p">:</span> <span class="n">EquiLinear</span>
</span><span id="MVOnlyGATrModel-381"><a href="#MVOnlyGATrModel-381"><span class="linenos">381</span></a>
</span><span id="MVOnlyGATrModel-382"><a href="#MVOnlyGATrModel-382"><span class="linenos">382</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrModel-383"><a href="#MVOnlyGATrModel-383"><span class="linenos">383</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MVOnlyGATrModel-384"><a href="#MVOnlyGATrModel-384"><span class="linenos">384</span></a>
</span><span id="MVOnlyGATrModel-385"><a href="#MVOnlyGATrModel-385"><span class="linenos">385</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MVOnlyGATrModel-386"><a href="#MVOnlyGATrModel-386"><span class="linenos">386</span></a>
</span><span id="MVOnlyGATrModel-387"><a href="#MVOnlyGATrModel-387"><span class="linenos">387</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">MVOnlyGATrEmbedding</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span id="MVOnlyGATrModel-388"><a href="#MVOnlyGATrModel-388"><span class="linenos">388</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="MVOnlyGATrModel-389"><a href="#MVOnlyGATrModel-389"><span class="linenos">389</span></a>            <span class="n">MVOnlyGATrBlock</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span>
</span><span id="MVOnlyGATrModel-390"><a href="#MVOnlyGATrModel-390"><span class="linenos">390</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrModel-391"><a href="#MVOnlyGATrModel-391"><span class="linenos">391</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_out</span><span class="p">)</span>
</span><span id="MVOnlyGATrModel-392"><a href="#MVOnlyGATrModel-392"><span class="linenos">392</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">)</span>
</span><span id="MVOnlyGATrModel-393"><a href="#MVOnlyGATrModel-393"><span class="linenos">393</span></a>
</span><span id="MVOnlyGATrModel-394"><a href="#MVOnlyGATrModel-394"><span class="linenos">394</span></a>    <span class="k">def</span> <span class="nf">_init_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="MVOnlyGATrModel-395"><a href="#MVOnlyGATrModel-395"><span class="linenos">395</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Slight adjustment to Kaiming init by down-scaling the weights</span>
</span><span id="MVOnlyGATrModel-396"><a href="#MVOnlyGATrModel-396"><span class="linenos">396</span></a><span class="sd">        by the number of encoder layers, following the GPT-2 paper.</span>
</span><span id="MVOnlyGATrModel-397"><a href="#MVOnlyGATrModel-397"><span class="linenos">397</span></a>
</span><span id="MVOnlyGATrModel-398"><a href="#MVOnlyGATrModel-398"><span class="linenos">398</span></a><span class="sd">        Parameters</span>
</span><span id="MVOnlyGATrModel-399"><a href="#MVOnlyGATrModel-399"><span class="linenos">399</span></a><span class="sd">        ----------</span>
</span><span id="MVOnlyGATrModel-400"><a href="#MVOnlyGATrModel-400"><span class="linenos">400</span></a><span class="sd">        module : nn.Module</span>
</span><span id="MVOnlyGATrModel-401"><a href="#MVOnlyGATrModel-401"><span class="linenos">401</span></a><span class="sd">            Module to initialize.</span>
</span><span id="MVOnlyGATrModel-402"><a href="#MVOnlyGATrModel-402"><span class="linenos">402</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MVOnlyGATrModel-403"><a href="#MVOnlyGATrModel-403"><span class="linenos">403</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">EquiLinear</span><span class="p">):</span>
</span><span id="MVOnlyGATrModel-404"><a href="#MVOnlyGATrModel-404"><span class="linenos">404</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
</span><span id="MVOnlyGATrModel-405"><a href="#MVOnlyGATrModel-405"><span class="linenos">405</span></a>            <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">/=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span>
</span><span id="MVOnlyGATrModel-406"><a href="#MVOnlyGATrModel-406"><span class="linenos">406</span></a>            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrModel-407"><a href="#MVOnlyGATrModel-407"><span class="linenos">407</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</span><span id="MVOnlyGATrModel-408"><a href="#MVOnlyGATrModel-408"><span class="linenos">408</span></a>
</span><span id="MVOnlyGATrModel-409"><a href="#MVOnlyGATrModel-409"><span class="linenos">409</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="MVOnlyGATrModel-410"><a href="#MVOnlyGATrModel-410"><span class="linenos">410</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MVOnlyGATrModel-411"><a href="#MVOnlyGATrModel-411"><span class="linenos">411</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MVOnlyGATrModel-412"><a href="#MVOnlyGATrModel-412"><span class="linenos">412</span></a>        <span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MVOnlyGATrModel-413"><a href="#MVOnlyGATrModel-413"><span class="linenos">413</span></a>        <span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MVOnlyGATrModel-414"><a href="#MVOnlyGATrModel-414"><span class="linenos">414</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MVOnlyGATrModel-415"><a href="#MVOnlyGATrModel-415"><span class="linenos">415</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span>
</span><span id="MVOnlyGATrModel-416"><a href="#MVOnlyGATrModel-416"><span class="linenos">416</span></a>            <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">block</span><span class="p">:</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reference</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">),</span>
</span><span id="MVOnlyGATrModel-417"><a href="#MVOnlyGATrModel-417"><span class="linenos">417</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">,</span>
</span><span id="MVOnlyGATrModel-418"><a href="#MVOnlyGATrModel-418"><span class="linenos">418</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
</span><span id="MVOnlyGATrModel-419"><a href="#MVOnlyGATrModel-419"><span class="linenos">419</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrModel-420"><a href="#MVOnlyGATrModel-420"><span class="linenos">420</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Multi-Vector only GATr model.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>config</strong> (MVOnlyGATrConfig):
Configuration object for the model. See <code><a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></code> for more details.</li>
</ul>
</div>


                            <div id="MVOnlyGATrModel.__init__" class="classattr">
                                        <input id="MVOnlyGATrModel.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">

        <span class="name">MVOnlyGATrModel</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">config</span><span class="p">:</span> <span class="n"><a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></span></span>)</span>

                <label class="view-source-button" for="MVOnlyGATrModel.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrModel.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrModel.__init__-382"><a href="#MVOnlyGATrModel.__init__-382"><span class="linenos">382</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MVOnlyGATrConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MVOnlyGATrModel.__init__-383"><a href="#MVOnlyGATrModel.__init__-383"><span class="linenos">383</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MVOnlyGATrModel.__init__-384"><a href="#MVOnlyGATrModel.__init__-384"><span class="linenos">384</span></a>
</span><span id="MVOnlyGATrModel.__init__-385"><a href="#MVOnlyGATrModel.__init__-385"><span class="linenos">385</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MVOnlyGATrModel.__init__-386"><a href="#MVOnlyGATrModel.__init__-386"><span class="linenos">386</span></a>
</span><span id="MVOnlyGATrModel.__init__-387"><a href="#MVOnlyGATrModel.__init__-387"><span class="linenos">387</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">MVOnlyGATrEmbedding</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span id="MVOnlyGATrModel.__init__-388"><a href="#MVOnlyGATrModel.__init__-388"><span class="linenos">388</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="MVOnlyGATrModel.__init__-389"><a href="#MVOnlyGATrModel.__init__-389"><span class="linenos">389</span></a>            <span class="n">MVOnlyGATrBlock</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span>
</span><span id="MVOnlyGATrModel.__init__-390"><a href="#MVOnlyGATrModel.__init__-390"><span class="linenos">390</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrModel.__init__-391"><a href="#MVOnlyGATrModel.__init__-391"><span class="linenos">391</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">EquiLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">size_channels_hidden</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">size_channels_out</span><span class="p">)</span>
</span><span id="MVOnlyGATrModel.__init__-392"><a href="#MVOnlyGATrModel.__init__-392"><span class="linenos">392</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="MVOnlyGATrModel.config" class="classattr">
                                <div class="attr variable">
            <span class="name">config</span><span class="annotation">: <a href="#MVOnlyGATrConfig">MVOnlyGATrConfig</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrModel.config"></a>



                            </div>
                            <div id="MVOnlyGATrModel.embedding" class="classattr">
                                <div class="attr variable">
            <span class="name">embedding</span><span class="annotation">: <a href="#MVOnlyGATrEmbedding">MVOnlyGATrEmbedding</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrModel.embedding"></a>



                            </div>
                            <div id="MVOnlyGATrModel.blocks" class="classattr">
                                <div class="attr variable">
            <span class="name">blocks</span><span class="annotation">: torch.nn.modules.container.ModuleList</span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrModel.blocks"></a>



                            </div>
                            <div id="MVOnlyGATrModel.head" class="classattr">
                                <div class="attr variable">
            <span class="name">head</span><span class="annotation">: <a href="../nn/modules.html#EquiLinear">ezgatr.nn.modules.EquiLinear</a></span>


    </div>
    <a class="headerlink" href="#MVOnlyGATrModel.head"></a>



                            </div>
                            <div id="MVOnlyGATrModel.forward" class="classattr">
                                        <input id="MVOnlyGATrModel.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">

        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="MVOnlyGATrModel.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MVOnlyGATrModel.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MVOnlyGATrModel.forward-409"><a href="#MVOnlyGATrModel.forward-409"><span class="linenos">409</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span><span id="MVOnlyGATrModel.forward-410"><a href="#MVOnlyGATrModel.forward-410"><span class="linenos">410</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MVOnlyGATrModel.forward-411"><a href="#MVOnlyGATrModel.forward-411"><span class="linenos">411</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="MVOnlyGATrModel.forward-412"><a href="#MVOnlyGATrModel.forward-412"><span class="linenos">412</span></a>        <span class="n">reference</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MVOnlyGATrModel.forward-413"><a href="#MVOnlyGATrModel.forward-413"><span class="linenos">413</span></a>        <span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MVOnlyGATrModel.forward-414"><a href="#MVOnlyGATrModel.forward-414"><span class="linenos">414</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="MVOnlyGATrModel.forward-415"><a href="#MVOnlyGATrModel.forward-415"><span class="linenos">415</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span>
</span><span id="MVOnlyGATrModel.forward-416"><a href="#MVOnlyGATrModel.forward-416"><span class="linenos">416</span></a>            <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">block</span><span class="p">:</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reference</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">),</span>
</span><span id="MVOnlyGATrModel.forward-417"><a href="#MVOnlyGATrModel.forward-417"><span class="linenos">417</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">,</span>
</span><span id="MVOnlyGATrModel.forward-418"><a href="#MVOnlyGATrModel.forward-418"><span class="linenos">418</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
</span><span id="MVOnlyGATrModel.forward-419"><a href="#MVOnlyGATrModel.forward-419"><span class="linenos">419</span></a>        <span class="p">)</span>
</span><span id="MVOnlyGATrModel.forward-420"><a href="#MVOnlyGATrModel.forward-420"><span class="linenos">420</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="MVOnlyGATrModel.dump_patches" class="variable">dump_patches</dd>
                <dd id="MVOnlyGATrModel.training" class="variable">training</dd>
                <dd id="MVOnlyGATrModel.call_super_init" class="variable">call_super_init</dd>
                <dd id="MVOnlyGATrModel.register_buffer" class="function">register_buffer</dd>
                <dd id="MVOnlyGATrModel.register_parameter" class="function">register_parameter</dd>
                <dd id="MVOnlyGATrModel.add_module" class="function">add_module</dd>
                <dd id="MVOnlyGATrModel.register_module" class="function">register_module</dd>
                <dd id="MVOnlyGATrModel.get_submodule" class="function">get_submodule</dd>
                <dd id="MVOnlyGATrModel.get_parameter" class="function">get_parameter</dd>
                <dd id="MVOnlyGATrModel.get_buffer" class="function">get_buffer</dd>
                <dd id="MVOnlyGATrModel.get_extra_state" class="function">get_extra_state</dd>
                <dd id="MVOnlyGATrModel.set_extra_state" class="function">set_extra_state</dd>
                <dd id="MVOnlyGATrModel.apply" class="function">apply</dd>
                <dd id="MVOnlyGATrModel.cuda" class="function">cuda</dd>
                <dd id="MVOnlyGATrModel.ipu" class="function">ipu</dd>
                <dd id="MVOnlyGATrModel.xpu" class="function">xpu</dd>
                <dd id="MVOnlyGATrModel.cpu" class="function">cpu</dd>
                <dd id="MVOnlyGATrModel.type" class="function">type</dd>
                <dd id="MVOnlyGATrModel.float" class="function">float</dd>
                <dd id="MVOnlyGATrModel.double" class="function">double</dd>
                <dd id="MVOnlyGATrModel.half" class="function">half</dd>
                <dd id="MVOnlyGATrModel.bfloat16" class="function">bfloat16</dd>
                <dd id="MVOnlyGATrModel.to_empty" class="function">to_empty</dd>
                <dd id="MVOnlyGATrModel.to" class="function">to</dd>
                <dd id="MVOnlyGATrModel.register_full_backward_pre_hook" class="function">register_full_backward_pre_hook</dd>
                <dd id="MVOnlyGATrModel.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="MVOnlyGATrModel.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="MVOnlyGATrModel.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="MVOnlyGATrModel.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="MVOnlyGATrModel.register_state_dict_pre_hook" class="function">register_state_dict_pre_hook</dd>
                <dd id="MVOnlyGATrModel.state_dict" class="function">state_dict</dd>
                <dd id="MVOnlyGATrModel.register_load_state_dict_post_hook" class="function">register_load_state_dict_post_hook</dd>
                <dd id="MVOnlyGATrModel.load_state_dict" class="function">load_state_dict</dd>
                <dd id="MVOnlyGATrModel.parameters" class="function">parameters</dd>
                <dd id="MVOnlyGATrModel.named_parameters" class="function">named_parameters</dd>
                <dd id="MVOnlyGATrModel.buffers" class="function">buffers</dd>
                <dd id="MVOnlyGATrModel.named_buffers" class="function">named_buffers</dd>
                <dd id="MVOnlyGATrModel.children" class="function">children</dd>
                <dd id="MVOnlyGATrModel.named_children" class="function">named_children</dd>
                <dd id="MVOnlyGATrModel.modules" class="function">modules</dd>
                <dd id="MVOnlyGATrModel.named_modules" class="function">named_modules</dd>
                <dd id="MVOnlyGATrModel.train" class="function">train</dd>
                <dd id="MVOnlyGATrModel.eval" class="function">eval</dd>
                <dd id="MVOnlyGATrModel.requires_grad_" class="function">requires_grad_</dd>
                <dd id="MVOnlyGATrModel.zero_grad" class="function">zero_grad</dd>
                <dd id="MVOnlyGATrModel.share_memory" class="function">share_memory</dd>
                <dd id="MVOnlyGATrModel.extra_repr" class="function">extra_repr</dd>
                <dd id="MVOnlyGATrModel.compile" class="function">compile</dd>

            </div>
                                </dl>
                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>
